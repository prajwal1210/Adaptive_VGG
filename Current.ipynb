{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/du0/15CS30043/anaconda3/envs/tensorflow/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from data_utility import *\n",
    "\n",
    "iterations      = 200\n",
    "batch_size      = 250\n",
    "total_epoch     = 164\n",
    "weight_decay    = 0.0003\n",
    "dropout_rate    = 0.5\n",
    "momentum_rate   = 0.9\n",
    "log_save_path   = './vgg_logs'\n",
    "model_save_path = './model/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================== #\n",
    "# ├─ bias_variable()\n",
    "# ├─ conv2d()           With Batch Normalization\n",
    "# ├─ max_pool()\n",
    "# └─ global_avg_pool()\n",
    "# ========================================================== #\n",
    "\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape, dtype=tf.float32 )\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1,1,1,1], padding='SAME')\n",
    "\n",
    "def max_pool(input, k_size=1, stride=1, name=None):\n",
    "    return tf.nn.max_pool(input, ksize=[1, k_size, k_size, 1], strides=[1, stride, stride, 1], padding='SAME',name=name)\n",
    "\n",
    "def batch_norm(input):\n",
    "    return tf.contrib.layers.batch_norm(input, decay=0.9, center=True, scale=True, epsilon=1e-3, is_training=train_flag, updates_collections=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================== #\n",
    "# ├─ _random_crop() \n",
    "# ├─ _random_flip_leftright()\n",
    "# ├─ data_augmentation()\n",
    "# ├─ data_preprocessing()\n",
    "# └─ learning_rate_schedule()\n",
    "# ========================================================== #\n",
    "\n",
    "def _random_crop(batch, crop_shape, padding=None):\n",
    "        oshape = np.shape(batch[0])\n",
    "        \n",
    "        if padding:\n",
    "            oshape = (oshape[0] + 2*padding, oshape[1] + 2*padding)\n",
    "        new_batch = []\n",
    "        npad = ((padding, padding), (padding, padding), (0, 0))\n",
    "        for i in range(len(batch)):\n",
    "            new_batch.append(batch[i])\n",
    "            if padding:\n",
    "                new_batch[i] = np.lib.pad(batch[i], pad_width=npad,\n",
    "                                          mode='constant', constant_values=0)\n",
    "            nh = random.randint(0, oshape[0] - crop_shape[0])\n",
    "            nw = random.randint(0, oshape[1] - crop_shape[1])\n",
    "            new_batch[i] = new_batch[i][nh:nh + crop_shape[0],\n",
    "                                        nw:nw + crop_shape[1]]\n",
    "        return new_batch\n",
    "\n",
    "def _random_flip_leftright(batch):\n",
    "        for i in range(len(batch)):\n",
    "            if bool(random.getrandbits(1)):\n",
    "                batch[i] = np.fliplr(batch[i])\n",
    "        return batch\n",
    "\n",
    "def data_preprocessing(x_train,x_test):\n",
    "\n",
    "    x_train = x_train.astype('float32')\n",
    "    x_test = x_test.astype('float32')\n",
    "\n",
    "    x_train[:,:,:,0] = (x_train[:,:,:,0] - np.mean(x_train[:,:,:,0])) / np.std(x_train[:,:,:,0])\n",
    "    x_train[:,:,:,1] = (x_train[:,:,:,1] - np.mean(x_train[:,:,:,1])) / np.std(x_train[:,:,:,1])\n",
    "    x_train[:,:,:,2] = (x_train[:,:,:,2] - np.mean(x_train[:,:,:,2])) / np.std(x_train[:,:,:,2])\n",
    "\n",
    "    x_test[:,:,:,0] = (x_test[:,:,:,0] - np.mean(x_test[:,:,:,0])) / np.std(x_test[:,:,:,0])\n",
    "    x_test[:,:,:,1] = (x_test[:,:,:,1] - np.mean(x_test[:,:,:,1])) / np.std(x_test[:,:,:,1])\n",
    "    x_test[:,:,:,2] = (x_test[:,:,:,2] - np.mean(x_test[:,:,:,2])) / np.std(x_test[:,:,:,2])\n",
    "\n",
    "    return x_train, x_test\n",
    "\n",
    "def learning_rate_schedule(epoch_num):\n",
    "      if epoch_num < 81:\n",
    "          return 0.1\n",
    "      elif epoch_num < 121:\n",
    "          return 0.01\n",
    "      else:\n",
    "          return 0.001\n",
    "\n",
    "def data_augmentation(batch):\n",
    "    batch = _random_flip_leftright(batch)\n",
    "    batch = _random_crop(batch, [32,32], 4)\n",
    "    return batch\n",
    "\n",
    "def run_testing(sess,ep,k_list):\n",
    "    acc = 0.0\n",
    "    loss = 0.0\n",
    "    pre_index = 0\n",
    "    add = 1000\n",
    "    for it in range(10):\n",
    "        batch_x = test_x[pre_index:pre_index+add]\n",
    "        batch_y = test_y[pre_index:pre_index+add]\n",
    "        pre_index = pre_index + add\n",
    "        loss_, acc_  = sess.run([cross_entropy,accuracy],feed_dict={x:batch_x, y_:batch_y, keep_prob: 1.0, train_flag: False, K:k_list})\n",
    "        loss += loss_ / 10.0\n",
    "        acc += acc_ / 10.0\n",
    "    summary = tf.Summary(value=[tf.Summary.Value(tag=\"test_loss\", simple_value=loss), \n",
    "                            tf.Summary.Value(tag=\"test_accuracy\", simple_value=acc)])\n",
    "    return acc, loss, summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================== #\n",
    "# ├─ main()\n",
    "# Training and Testing \n",
    "# Save train/teset loss and acc for visualization\n",
    "# Save Model in ./model\n",
    "# ========================================================== #\n",
    "\n",
    "#Quantization functions\n",
    "G = tf.get_default_graph()\n",
    "def quantize(x):\n",
    "    with G.gradient_override_map({\"Sign\": \"Identity\"}):\n",
    "        return tf.sign(x)\n",
    "\n",
    "\n",
    "def quantize_2(x):\n",
    "    with G.gradient_override_map({\"Sign\": \"Identity\", \"Where\":\"Identity\"}):\n",
    "        result = tf.sign(x)*tf.where(tf.abs(x)<= 0.75, tf.ones_like(x)/2,tf.ones_like(x))\n",
    "        return result\n",
    "\n",
    "    \n",
    "def quantization(x,k):\n",
    "    return tf.cond(tf.equal(k,1),lambda: quantize(x),lambda: quantize_2(x))    \n",
    "\n",
    "\n",
    "#Calculating Regularization Loss\n",
    "def reg_1(x):\n",
    "    return tf.reduce_sum(tf.abs(tf.add(tf.ones_like(x),tf.negative(tf.square(x)))))\n",
    "    \n",
    "\n",
    "def reg_2(x):\n",
    "    return tf.reduce_sum(tf.abs(tf.multiply(tf.add(tf.ones_like(x),tf.negative(tf.square(x))),tf.add(tf.ones_like(x)/4,tf.negative(tf.square(x))))))\n",
    "    \n",
    "def reg_loss_fn(x,k):\n",
    "    return tf.cond(tf.equal(k,1),lambda: reg_1(x),lambda: reg_2(x))    \n",
    "\n",
    "def mask_1(x,epsilon):\n",
    "    return tf.reduce_mean(tf.abs(tf.where(tf.less(tf.abs(tf.subtract(tf.abs(x),tf.ones_like(x))),epsilon),tf.sign(x), tf.zeros_like(x))))\n",
    "    \n",
    "\n",
    "def mask_2(x,epsilon):\n",
    "    return tf.reduce_mean(tf.abs(tf.where(tf.logical_or(tf.less(tf.abs(tf.subtract(tf.abs(x),tf.ones_like(x))),epsilon),tf.less(tf.abs(tf.subtract(tf.abs(x),tf.ones_like(x)/2)),epsilon)),tf.sign(x), tf.zeros_like(x))))\n",
    "\n",
    "def mask_fn(x,k,epsilon):\n",
    "    return tf.cond(tf.equal(k,1),lambda: mask_1(x,epsilon),lambda: mask_2(x,epsilon))    \n",
    "\n",
    "def update_k(masks,k_list):\n",
    "    threshold = 0.10\n",
    "    for i in range(len(masks)):\n",
    "        if(masks[i] < threshold):\n",
    "            k_list[i] = min(k_list[i]+1,2)\n",
    "    return k_list\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======Loading data======\n",
      "DataSet aready exist!\n",
      "Loading ./cifar-10-batches-py/data_batch_1 : 10000.\n",
      "Loading ./cifar-10-batches-py/data_batch_2 : 10000.\n",
      "Loading ./cifar-10-batches-py/data_batch_3 : 10000.\n",
      "Loading ./cifar-10-batches-py/data_batch_4 : 10000.\n",
      "Loading ./cifar-10-batches-py/data_batch_5 : 10000.\n",
      "Loading ./cifar-10-batches-py/test_batch : 10000.\n",
      "Train data: (50000, 32, 32, 3) (50000, 10)\n",
      "Test data : (10000, 32, 32, 3) (10000, 10)\n",
      "======Load finished======\n",
      "======Shuffling data======\n",
      "======Prepare Finished======\n",
      "\n",
      "epoch 1/164:\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    train_x, train_y, test_x, test_y = prepare_data()\n",
    "    train_x, test_x = data_preprocessing(train_x, test_x)\n",
    "\n",
    "    # define placeholder x, y_ , keep_prob, learning_rate\n",
    "    x  = tf.placeholder(tf.float32,[None, image_size, image_size, 3])\n",
    "    y_ = tf.placeholder(tf.float32, [None, class_num])\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    learning_rate = tf.placeholder(tf.float32)\n",
    "    train_flag = tf.placeholder(tf.bool)\n",
    "\n",
    "    # build_network\n",
    "    net_weights = {}\n",
    "    \n",
    "    num_weights = 19\n",
    "    K = tf.placeholder(dtype = tf.int32, shape = (num_weights,))\n",
    "    \n",
    "    \n",
    "    net_weights['W_conv1_1'] = {}\n",
    "    net_weights['W_conv1_1']['w'] = tf.get_variable('conv1_1', shape=[3, 3, 3, 64], initializer=tf.contrib.keras.initializers.he_normal())\n",
    "    b_conv1_1 = bias_variable([64])\n",
    "    W_conv1_1_sign = quantization(net_weights['W_conv1_1']['w'],K[0])\n",
    "    output  = tf.nn.relu( batch_norm(conv2d(x,W_conv1_1_sign) + b_conv1_1))\n",
    "    \n",
    "    net_weights['W_conv1_2'] = {}\n",
    "    net_weights['W_conv1_2']['w'] = tf.get_variable('conv1_2', shape=[3, 3, 64, 64], initializer=tf.contrib.keras.initializers.he_normal())\n",
    "    b_conv1_2 = bias_variable([64])\n",
    "    W_conv1_2_sign = quantization(net_weights['W_conv1_2']['w'],K[1])\n",
    "    output  = tf.nn.relu( batch_norm(conv2d(output,W_conv1_2_sign) + b_conv1_2))\n",
    "    output  = max_pool(output, 2, 2, \"pool1\")\n",
    "    \n",
    "    net_weights['W_conv2_1'] = {}\n",
    "    net_weights['W_conv2_1']['w'] = tf.get_variable('conv2_1', shape=[3, 3, 64, 128], initializer=tf.contrib.keras.initializers.he_normal())\n",
    "    b_conv2_1 = bias_variable([128])\n",
    "    W_conv2_1_sign = quantization(net_weights['W_conv2_1']['w'],K[2])\n",
    "    output  = tf.nn.relu( batch_norm(conv2d(output,W_conv2_1_sign) + b_conv2_1))\n",
    "    \n",
    "    net_weights['W_conv2_2'] = {}\n",
    "    net_weights['W_conv2_2']['w'] = tf.get_variable('conv2_2', shape=[3, 3, 128, 128], initializer=tf.contrib.keras.initializers.he_normal())\n",
    "    b_conv2_2 = bias_variable([128])\n",
    "    W_conv2_2_sign = quantization(net_weights['W_conv2_2']['w'],K[3])\n",
    "    output  = tf.nn.relu( batch_norm(conv2d(output,W_conv2_2_sign) + b_conv2_2))\n",
    "    output  = max_pool(output, 2, 2, \"pool2\")\n",
    "    \n",
    "    net_weights['W_conv3_1'] = {}\n",
    "    net_weights['W_conv3_1']['w'] = tf.get_variable('conv3_1', shape=[3, 3, 128, 256], initializer=tf.contrib.keras.initializers.he_normal())\n",
    "    b_conv3_1 = bias_variable([256])\n",
    "    W_conv3_1_sign = quantization(net_weights['W_conv3_1']['w'],K[4])\n",
    "    output  = tf.nn.relu( batch_norm(conv2d(output,W_conv3_1_sign) + b_conv3_1))\n",
    "\n",
    "    net_weights['W_conv3_2'] = {}\n",
    "    net_weights['W_conv3_2']['w'] = tf.get_variable('conv3_2', shape=[3, 3, 256, 256], initializer=tf.contrib.keras.initializers.he_normal())\n",
    "    b_conv3_2 = bias_variable([256])\n",
    "    W_conv3_2_sign = quantization(net_weights['W_conv3_2']['w'],K[5])\n",
    "    output  = tf.nn.relu( batch_norm(conv2d(output,W_conv3_2_sign) + b_conv3_2))\n",
    "\n",
    "    net_weights['W_conv3_3'] = {}\n",
    "    net_weights['W_conv3_3']['w'] = tf.get_variable('conv3_3', shape=[3, 3, 256, 256], initializer=tf.contrib.keras.initializers.he_normal())\n",
    "    b_conv3_3 = bias_variable([256])\n",
    "    W_conv3_3_sign = quantization(net_weights['W_conv3_3']['w'],K[6])\n",
    "    output  = tf.nn.relu( batch_norm(conv2d(output,W_conv3_3_sign) + b_conv3_3))\n",
    "\n",
    "    net_weights['W_conv3_4'] = {}\n",
    "    net_weights['W_conv3_4']['w'] = tf.get_variable('conv3_4', shape=[3, 3, 256, 256], initializer=tf.contrib.keras.initializers.he_normal())\n",
    "    b_conv3_4 = bias_variable([256])\n",
    "    W_conv3_4_sign = quantization(net_weights['W_conv3_4']['w'],K[7])\n",
    "    output  = tf.nn.relu( batch_norm(conv2d(output,W_conv3_4_sign) + b_conv3_4))\n",
    "    output  = max_pool(output, 2, 2, \"pool3\")\n",
    "\n",
    "    net_weights['W_conv4_1'] = {}\n",
    "    net_weights['W_conv4_1']['w'] = tf.get_variable('conv4_1', shape=[3, 3, 256, 512], initializer=tf.contrib.keras.initializers.he_normal())\n",
    "    b_conv4_1 = bias_variable([512])\n",
    "    W_conv4_1_sign = quantization(net_weights['W_conv4_1']['w'],K[8])\n",
    "    output  = tf.nn.relu( batch_norm(conv2d(output,W_conv4_1_sign) + b_conv4_1))\n",
    "\n",
    "    net_weights['W_conv4_2'] = {}\n",
    "    net_weights['W_conv4_2']['w'] = tf.get_variable('conv4_2', shape=[3, 3, 512, 512], initializer=tf.contrib.keras.initializers.he_normal())\n",
    "    b_conv4_2 = bias_variable([512])\n",
    "    W_conv4_2_sign = quantization(net_weights['W_conv4_2']['w'],K[9])\n",
    "    output  = tf.nn.relu( batch_norm(conv2d(output,W_conv4_2_sign) + b_conv4_2))\n",
    "    \n",
    "    net_weights['W_conv4_3'] = {}\n",
    "    net_weights['W_conv4_3']['w'] = tf.get_variable('conv4_3', shape=[3, 3, 512, 512], initializer=tf.contrib.keras.initializers.he_normal())\n",
    "    b_conv4_3 = bias_variable([512])\n",
    "    W_conv4_3_sign = quantization(net_weights['W_conv4_3']['w'],K[10])\n",
    "    output  = tf.nn.relu( batch_norm(conv2d(output,W_conv4_3_sign) + b_conv4_3))\n",
    "    \n",
    "    net_weights['W_conv4_4'] = {}\n",
    "    net_weights['W_conv4_4']['w'] = tf.get_variable('conv4_4', shape=[3, 3, 512, 512], initializer=tf.contrib.keras.initializers.he_normal())\n",
    "    b_conv4_4 = bias_variable([512])\n",
    "    W_conv4_4_sign = quantization(net_weights['W_conv4_4']['w'],K[11])\n",
    "    output  = tf.nn.relu( batch_norm(conv2d(output,W_conv4_4_sign)) + b_conv4_4)\n",
    "    output  = max_pool(output, 2, 2)\n",
    "    \n",
    "    net_weights['W_conv5_1'] = {}\n",
    "    net_weights['W_conv5_1']['w'] = tf.get_variable('conv5_1', shape=[3, 3, 512, 512], initializer=tf.contrib.keras.initializers.he_normal())\n",
    "    b_conv5_1 = bias_variable([512])\n",
    "    W_conv5_1_sign = quantization(net_weights['W_conv5_1']['w'],K[12])\n",
    "    output  = tf.nn.relu( batch_norm(conv2d(output,W_conv5_1_sign) + b_conv5_1))\n",
    "    \n",
    "    net_weights['W_conv5_2'] = {}\n",
    "    net_weights['W_conv5_2']['w']= tf.get_variable('conv5_2', shape=[3, 3, 512, 512], initializer=tf.contrib.keras.initializers.he_normal())\n",
    "    b_conv5_2 = bias_variable([512])\n",
    "    W_conv5_2_sign = quantization(net_weights['W_conv5_2']['w'],K[13])\n",
    "    output  = tf.nn.relu( batch_norm(conv2d(output,W_conv5_2_sign) + b_conv5_2))\n",
    "    \n",
    "    net_weights['W_conv5_3'] = {}\n",
    "    net_weights['W_conv5_3']['w'] = tf.get_variable('conv5_3', shape=[3, 3, 512, 512], initializer=tf.contrib.keras.initializers.he_normal())\n",
    "    b_conv5_3 = bias_variable([512])\n",
    "    W_conv5_3_sign = quantization(net_weights['W_conv5_3']['w'],K[14])\n",
    "    output  = tf.nn.relu( batch_norm(conv2d(output,W_conv5_3_sign) + b_conv5_3))\n",
    "    \n",
    "    net_weights['W_conv5_4'] = {}\n",
    "    net_weights['W_conv5_4']['w'] = tf.get_variable('conv5_4', shape=[3, 3, 512, 512], initializer=tf.contrib.keras.initializers.he_normal())\n",
    "    b_conv5_4 = bias_variable([512])\n",
    "    W_conv5_4_sign = quantization(net_weights['W_conv5_4']['w'],K[15])\n",
    "    output  = tf.nn.relu( batch_norm(conv2d(output,W_conv5_4_sign) + b_conv5_4))\n",
    "\n",
    "    # output = tf.contrib.layers.flatten(output)\n",
    "    output = tf.reshape(output,[-1,2*2*512])\n",
    "    \n",
    "    net_weights['W_fc1'] = {}\n",
    "    net_weights['W_fc1']['w'] = tf.get_variable('fc1', shape=[2048,4096], initializer=tf.contrib.keras.initializers.he_normal())\n",
    "    b_fc1 = bias_variable([4096])\n",
    "    W_fc1_sign = quantization(net_weights['W_fc1']['w'],K[16])\n",
    "    output = tf.nn.relu( batch_norm(tf.matmul(output,W_fc1_sign) + b_fc1) )\n",
    "    output  = tf.nn.dropout(output,keep_prob)\n",
    "    \n",
    "    net_weights['W_fc2'] = {}\n",
    "    net_weights['W_fc2']['w'] = tf.get_variable('fc7', shape=[4096,4096], initializer=tf.contrib.keras.initializers.he_normal())\n",
    "    b_fc2 = bias_variable([4096])\n",
    "    W_fc2_sign = quantization(net_weights['W_fc2']['w'],K[17])\n",
    "    output = tf.nn.relu( batch_norm(tf.matmul(output,W_fc2_sign) + b_fc2) )\n",
    "    output  = tf.nn.dropout(output,keep_prob)\n",
    "\n",
    "    net_weights['W_fc3'] = {}\n",
    "    net_weights['W_fc3']['w'] = tf.get_variable('fc3', shape=[4096,10], initializer=tf.contrib.keras.initializers.he_normal())\n",
    "    b_fc3 = bias_variable([10])\n",
    "    W_fc3_sign = quantization(net_weights['W_fc3']['w'],K[18])\n",
    "    output = tf.nn.relu( batch_norm(tf.matmul(output,W_fc3_sign) + b_fc3) )\n",
    "    # output  = tf.reshape(output,[-1,10])\n",
    "\n",
    "    reg_loss = 0\n",
    "    weights = []\n",
    "    for key in net_weights.keys():\n",
    "        weights.append(net_weights[key]['w'])\n",
    "#     weights = [W_conv1_1,W_conv1_2,W_conv2_1,W_conv2_2,W_conv3_1,W_conv3_2,W_conv3_3,W_conv3_4,W_conv4_1,W_conv4_1,W_conv4_2,W_conv4_3,W_conv4_4,W_conv5_1,W_conv5_2,W_conv5_3,W_conv5_4,W_fc1,W_fc2,W_fc3]\n",
    "    # for weight in weights:\n",
    "    #     reg_loss+= tf.reduce_sum(tf.abs(tf.add(tf.ones_like(weight),tf.negative(tf.square(weight)))))\n",
    "    reg_loss = reg_loss_fn(net_weights['W_conv1_1']['w'], K[0]) \\\n",
    "            +  reg_loss_fn(net_weights['W_conv1_2']['w'], K[1]) \\\n",
    "            +  reg_loss_fn(net_weights['W_conv2_1']['w'], K[2]) \\\n",
    "            +  reg_loss_fn(net_weights['W_conv2_2']['w'], K[3]) \\\n",
    "            +  reg_loss_fn(net_weights['W_conv3_1']['w'], K[4]) \\\n",
    "            +  reg_loss_fn(net_weights['W_conv3_2']['w'], K[5]) \\\n",
    "            +  reg_loss_fn(net_weights['W_conv3_3']['w'], K[6]) \\\n",
    "            +  reg_loss_fn(net_weights['W_conv3_4']['w'], K[7]) \\\n",
    "            +  reg_loss_fn(net_weights['W_conv4_1']['w'], K[8]) \\\n",
    "            +  reg_loss_fn(net_weights['W_conv4_2']['w'], K[9]) \\\n",
    "            +  reg_loss_fn(net_weights['W_conv4_3']['w'], K[10]) \\\n",
    "            +  reg_loss_fn(net_weights['W_conv4_4']['w'], K[11]) \\\n",
    "            +  reg_loss_fn(net_weights['W_conv5_1']['w'], K[12]) \\\n",
    "            +  reg_loss_fn(net_weights['W_conv5_2']['w'], K[13]) \\\n",
    "            +  reg_loss_fn(net_weights['W_conv5_3']['w'], K[14]) \\\n",
    "            +  reg_loss_fn(net_weights['W_conv5_4']['w'], K[15]) \\\n",
    "            +  reg_loss_fn(net_weights['W_fc1']['w'],K[16]) \\\n",
    "            +  reg_loss_fn(net_weights['W_fc2']['w'],K[17]) \\\n",
    "            +  reg_loss_fn(net_weights['W_fc3']['w'],K[18]) \\\n",
    "\n",
    "\n",
    "\n",
    "    # loss function: cross_entropy\n",
    "    # train_step: training operation\n",
    "    cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=output))\n",
    "    l2 = tf.add_n([tf.nn.l2_loss(var) for var in tf.trainable_variables()])\n",
    "    lbda = tf.placeholder(tf.float32)\n",
    "    train_step = tf.train.MomentumOptimizer(learning_rate, momentum_rate,use_nesterov=True).minimize(cross_entropy + lbda*reg_loss)\n",
    "    \n",
    "    correct_prediction = tf.equal(tf.argmax(output,1), tf.argmax(y_,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
    "\n",
    "    #Masks\n",
    "    epsilon = 0.00001\n",
    "    mask = [mask_fn(weights[i],K[i],epsilon) for i in range(len(weights))]\n",
    "    \n",
    "    # epsilon = 0.00001\n",
    "    # for i in weights:\n",
    "    #     mask.append(tf.reduce_mean(tf.abs(tf.where(tf.less(tf.abs(tf.subtract(tf.abs(i),tf.ones_like(i))),epsilon),tf.sign(i), tf.zeros_like(i)))))\n",
    "\n",
    "\n",
    "    #Binarization \n",
    "    binary = []\n",
    "    for i in weights:\n",
    "        binary.append(tf.assign(i, tf.sign(i)))\n",
    "\n",
    "\n",
    "    list_bin = []\n",
    "\n",
    "\n",
    "    # initial an saver to save model\n",
    "    saver = tf.train.Saver()\n",
    "    config=tf.ConfigProto(allow_soft_placement=True, log_device_placement=True)\n",
    "    config.gpu_options.allow_growth = True\n",
    "    with tf.Session(config = config) as sess:\n",
    "        \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        summary_writer = tf.summary.FileWriter(log_save_path,sess.graph)\n",
    "\n",
    "        # epoch = 164 \n",
    "        # make sure [bath_size * iteration = data_set_number]\n",
    "        k_list = []\n",
    "        for i in range(num_weights):\n",
    "            k_list.append(2)\n",
    "        for ep in range(1,total_epoch+1):\n",
    "            lr = learning_rate_schedule(ep)\n",
    "            pre_index = 0\n",
    "            train_acc = 0.0\n",
    "            train_loss = 0.0\n",
    "            start_time = time.time()\n",
    "            print(\"\\nepoch %d/%d:\" %(ep,total_epoch))\n",
    "            masks = None\n",
    "            for it in range(1,iterations+1):\n",
    "                batch_x = train_x[pre_index:pre_index+batch_size]\n",
    "                batch_y = train_y[pre_index:pre_index+batch_size]\n",
    "\n",
    "                batch_x = data_augmentation(batch_x)\n",
    "\n",
    "                _, batch_loss = sess.run([train_step, cross_entropy],feed_dict={x:batch_x, y_:batch_y, keep_prob: dropout_rate, learning_rate: lr, train_flag: True, lbda:0.01, K:k_list})\n",
    "                batch_acc = accuracy.eval(feed_dict={x:batch_x, y_:batch_y, keep_prob: 1.0, train_flag: True, K:k_list})\n",
    "\n",
    "                train_loss += batch_loss\n",
    "                train_acc  += batch_acc\n",
    "                pre_index  += batch_size\n",
    "\n",
    "                if it == iterations:\n",
    "                    train_loss /= iterations\n",
    "                    train_acc /= iterations\n",
    "\n",
    "                    loss_, acc_  = sess.run([cross_entropy,accuracy],feed_dict={x:batch_x, y_:batch_y, keep_prob: 1.0, train_flag: True, lbda:0.01, K:k_list})\n",
    "                    train_summary = tf.Summary(value=[tf.Summary.Value(tag=\"train_loss\", simple_value=train_loss), \n",
    "                                          tf.Summary.Value(tag=\"train_accuracy\", simple_value=train_acc)])\n",
    "                    val_acc, val_loss, test_summary = run_testing(sess,ep,k_list)\n",
    "\n",
    "                    summary_writer.add_summary(train_summary, ep)\n",
    "                    summary_writer.add_summary(test_summary, ep)\n",
    "                    summary_writer.flush()\n",
    "\n",
    "                    print(\"iteration: %d/%d, cost_time: %ds, train_loss: %.4f, train_acc: %.4f, test_loss: %.4f, test_acc: %.4f\" %(it, iterations, int(time.time()-start_time), train_loss, train_acc, val_loss, val_acc))\n",
    "                    list_bin.append(sess.run(mask, feed_dict = {K:k_list}))\n",
    "                    masks = sess.run(mask, feed_dict = {K:k_list})\n",
    "#                     print (sess.run(weights))\n",
    "                else:\n",
    "                    print(\"iteration: %d/%d, train_loss: %.4f, train_acc: %.4f\" %(it, iterations, train_loss / it, train_acc / it) , end='\\r')\n",
    "            if(ep%10 == 0):\n",
    "                print(masks)\n",
    "                k_list = update_k(masks,k_list)\n",
    "            print (k_list)\n",
    "        save_path = saver.save(sess, model_save_path)\n",
    "        print(\"Model saved in file: %s\" % save_path)\n",
    "        print (list_bin)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
