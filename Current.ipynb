{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/du0/15CS30043/anaconda3/envs/tensorflow/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from data_utility import *\n",
    "\n",
    "iterations      = 200\n",
    "batch_size      = 250\n",
    "total_epoch     = 164\n",
    "weight_decay    = 0.0003\n",
    "dropout_rate    = 0.5\n",
    "momentum_rate   = 0.9\n",
    "log_save_path   = './vgg_logs'\n",
    "model_save_path = './model/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================== #\n",
    "# ├─ bias_variable()\n",
    "# ├─ conv2d()           With Batch Normalization\n",
    "# ├─ max_pool()\n",
    "# └─ global_avg_pool()\n",
    "# ========================================================== #\n",
    "\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape, dtype=tf.float32 )\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1,1,1,1], padding='SAME')\n",
    "\n",
    "def max_pool(input, k_size=1, stride=1, name=None):\n",
    "    return tf.nn.max_pool(input, ksize=[1, k_size, k_size, 1], strides=[1, stride, stride, 1], padding='SAME',name=name)\n",
    "\n",
    "def batch_norm(input):\n",
    "    return tf.contrib.layers.batch_norm(input, decay=0.9, center=True, scale=True, epsilon=1e-3, is_training=train_flag, updates_collections=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================== #\n",
    "# ├─ _random_crop() \n",
    "# ├─ _random_flip_leftright()\n",
    "# ├─ data_augmentation()\n",
    "# ├─ data_preprocessing()\n",
    "# └─ learning_rate_schedule()\n",
    "# ========================================================== #\n",
    "\n",
    "def _random_crop(batch, crop_shape, padding=None):\n",
    "        oshape = np.shape(batch[0])\n",
    "        \n",
    "        if padding:\n",
    "            oshape = (oshape[0] + 2*padding, oshape[1] + 2*padding)\n",
    "        new_batch = []\n",
    "        npad = ((padding, padding), (padding, padding), (0, 0))\n",
    "        for i in range(len(batch)):\n",
    "            new_batch.append(batch[i])\n",
    "            if padding:\n",
    "                new_batch[i] = np.lib.pad(batch[i], pad_width=npad,\n",
    "                                          mode='constant', constant_values=0)\n",
    "            nh = random.randint(0, oshape[0] - crop_shape[0])\n",
    "            nw = random.randint(0, oshape[1] - crop_shape[1])\n",
    "            new_batch[i] = new_batch[i][nh:nh + crop_shape[0],\n",
    "                                        nw:nw + crop_shape[1]]\n",
    "        return new_batch\n",
    "\n",
    "def _random_flip_leftright(batch):\n",
    "        for i in range(len(batch)):\n",
    "            if bool(random.getrandbits(1)):\n",
    "                batch[i] = np.fliplr(batch[i])\n",
    "        return batch\n",
    "\n",
    "def data_preprocessing(x_train,x_test):\n",
    "\n",
    "    x_train = x_train.astype('float32')\n",
    "    x_test = x_test.astype('float32')\n",
    "\n",
    "    x_train[:,:,:,0] = (x_train[:,:,:,0] - np.mean(x_train[:,:,:,0])) / np.std(x_train[:,:,:,0])\n",
    "    x_train[:,:,:,1] = (x_train[:,:,:,1] - np.mean(x_train[:,:,:,1])) / np.std(x_train[:,:,:,1])\n",
    "    x_train[:,:,:,2] = (x_train[:,:,:,2] - np.mean(x_train[:,:,:,2])) / np.std(x_train[:,:,:,2])\n",
    "\n",
    "    x_test[:,:,:,0] = (x_test[:,:,:,0] - np.mean(x_test[:,:,:,0])) / np.std(x_test[:,:,:,0])\n",
    "    x_test[:,:,:,1] = (x_test[:,:,:,1] - np.mean(x_test[:,:,:,1])) / np.std(x_test[:,:,:,1])\n",
    "    x_test[:,:,:,2] = (x_test[:,:,:,2] - np.mean(x_test[:,:,:,2])) / np.std(x_test[:,:,:,2])\n",
    "\n",
    "    return x_train, x_test\n",
    "\n",
    "def learning_rate_schedule(epoch_num):\n",
    "      if epoch_num < 81:\n",
    "          return 0.1\n",
    "      elif epoch_num < 121:\n",
    "          return 0.01\n",
    "      else:\n",
    "          return 0.001\n",
    "\n",
    "def data_augmentation(batch):\n",
    "    batch = _random_flip_leftright(batch)\n",
    "    batch = _random_crop(batch, [32,32], 4)\n",
    "    return batch\n",
    "\n",
    "def run_testing(sess,ep,k_list):\n",
    "    acc = 0.0\n",
    "    loss = 0.0\n",
    "    pre_index = 0\n",
    "    add = 1000\n",
    "    for it in range(10):\n",
    "        batch_x = test_x[pre_index:pre_index+add]\n",
    "        batch_y = test_y[pre_index:pre_index+add]\n",
    "        pre_index = pre_index + add\n",
    "        loss_, acc_  = sess.run([cross_entropy,accuracy],feed_dict={x:batch_x, y_:batch_y, keep_prob: 1.0, train_flag: False, K:k_list})\n",
    "        loss += loss_ / 10.0\n",
    "        acc += acc_ / 10.0\n",
    "    summary = tf.Summary(value=[tf.Summary.Value(tag=\"test_loss\", simple_value=loss), \n",
    "                            tf.Summary.Value(tag=\"test_accuracy\", simple_value=acc)])\n",
    "    return acc, loss, summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================== #\n",
    "# ├─ main()\n",
    "# Training and Testing \n",
    "# Save train/teset loss and acc for visualization\n",
    "# Save Model in ./model\n",
    "# ========================================================== #\n",
    "\n",
    "#Quantization functions\n",
    "G = tf.get_default_graph()\n",
    "def quantize(x):\n",
    "    with G.gradient_override_map({\"Sign\": \"Identity\"}):\n",
    "        return tf.sign(x)\n",
    "\n",
    "\n",
    "def quantize_2(x):\n",
    "    with G.gradient_override_map({\"Sign\": \"Identity\", \"Where\":\"Identity\", \"OnesLike\":\"Identity\"}):\n",
    "        result = tf.sign(x)*tf.where(tf.abs(x)<= 0.75, tf.ones_like(x)/2,tf.ones_like(x))\n",
    "        return result\n",
    "\n",
    "    \n",
    "def quantization(x,k):\n",
    "    return tf.cond(tf.equal(k,1),lambda: quantize(x),lambda: quantize_2(x))    \n",
    "\n",
    "\n",
    "#Calculating Regularization Loss\n",
    "def reg_1(x):\n",
    "    return tf.reduce_sum(tf.abs(tf.add(tf.ones_like(x),tf.negative(tf.square(x)))))\n",
    "    \n",
    "\n",
    "def reg_2(x):\n",
    "    return tf.reduce_sum(tf.abs(tf.multiply(tf.add(tf.ones_like(x),tf.negative(tf.square(x))),tf.add(tf.ones_like(x)/4,tf.negative(tf.square(x))))))\n",
    "    \n",
    "def reg_loss_fn(x,k):\n",
    "    return tf.cond(tf.equal(k,1),lambda: reg_1(x),lambda: reg_2(x))    \n",
    "\n",
    "def mask_1(x,epsilon):\n",
    "    return tf.reduce_mean(tf.abs(tf.where(tf.less(tf.abs(tf.subtract(tf.abs(x),tf.ones_like(x))),epsilon),tf.sign(x), tf.zeros_like(x))))\n",
    "    \n",
    "\n",
    "def mask_2(x,epsilon):\n",
    "    return tf.reduce_mean(tf.abs(tf.where(tf.logical_or(tf.less(tf.abs(tf.subtract(tf.abs(x),tf.ones_like(x))),epsilon),tf.less(tf.abs(tf.subtract(tf.abs(x),tf.ones_like(x)/2)),epsilon)),tf.sign(x), tf.zeros_like(x))))\n",
    "\n",
    "def mask_fn(x,k,epsilon):\n",
    "    return tf.cond(tf.equal(k,1),lambda: mask_1(x,epsilon),lambda: mask_2(x,epsilon))    \n",
    "\n",
    "def update_k(masks,k_list):\n",
    "    threshold = 0.10\n",
    "    for i in range(len(masks)):\n",
    "        if(masks[i] < threshold):\n",
    "            k_list[i] = min(k_list[i]+1,2)\n",
    "    return k_list\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======Loading data======\n",
      "DataSet aready exist!\n",
      "Loading ./cifar-10-batches-py/data_batch_1 : 10000.\n",
      "Loading ./cifar-10-batches-py/data_batch_2 : 10000.\n",
      "Loading ./cifar-10-batches-py/data_batch_3 : 10000.\n",
      "Loading ./cifar-10-batches-py/data_batch_4 : 10000.\n",
      "Loading ./cifar-10-batches-py/data_batch_5 : 10000.\n",
      "Loading ./cifar-10-batches-py/test_batch : 10000.\n",
      "Train data: (50000, 32, 32, 3) (50000, 10)\n",
      "Test data : (10000, 32, 32, 3) (10000, 10)\n",
      "======Load finished======\n",
      "======Shuffling data======\n",
      "======Prepare Finished======\n",
      "\n",
      "epoch 1/164:\n",
      "iteration: 200/200, cost_time: 181s, train_loss: 2.1806, train_acc: 0.2499, test_loss: 1.9609, test_acc: 0.3076\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "epoch 2/164:\n",
      "iteration: 200/200, cost_time: 137s, train_loss: 2.0171, train_acc: 0.3110, test_loss: 1.8649, test_acc: 0.3324\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "epoch 3/164:\n",
      "iteration: 200/200, cost_time: 136s, train_loss: 1.9508, train_acc: 0.3317, test_loss: 1.8004, test_acc: 0.3571\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "epoch 4/164:\n",
      "iteration: 200/200, cost_time: 138s, train_loss: 1.8961, train_acc: 0.3499, test_loss: 1.7496, test_acc: 0.3816\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "epoch 5/164:\n",
      "iteration: 200/200, cost_time: 136s, train_loss: 1.8508, train_acc: 0.3615, test_loss: 1.6975, test_acc: 0.3919\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "epoch 6/164:\n",
      "iteration: 200/200, cost_time: 142s, train_loss: 1.8091, train_acc: 0.3781, test_loss: 1.6418, test_acc: 0.4035\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "epoch 7/164:\n",
      "iteration: 200/200, cost_time: 137s, train_loss: 1.7610, train_acc: 0.3905, test_loss: 1.5947, test_acc: 0.4153\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "epoch 8/164:\n",
      "iteration: 200/200, cost_time: 137s, train_loss: 1.7326, train_acc: 0.4034, test_loss: 1.5674, test_acc: 0.4212\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "epoch 9/164:\n",
      "iteration: 200/200, cost_time: 137s, train_loss: 1.6987, train_acc: 0.4160, test_loss: 1.5354, test_acc: 0.4334\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "epoch 10/164:\n",
      "iteration: 200/200, cost_time: 136s, train_loss: 1.6726, train_acc: 0.4240, test_loss: 1.5102, test_acc: 0.4417\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "epoch 11/164:\n",
      "iteration: 200/200, cost_time: 140s, train_loss: 1.6450, train_acc: 0.4323, test_loss: 1.4955, test_acc: 0.4505\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "epoch 12/164:\n",
      "iteration: 200/200, cost_time: 136s, train_loss: 1.6197, train_acc: 0.4409, test_loss: 1.4657, test_acc: 0.4590\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "epoch 13/164:\n",
      "iteration: 200/200, cost_time: 136s, train_loss: 1.6025, train_acc: 0.4476, test_loss: 1.4453, test_acc: 0.4706\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "epoch 14/164:\n",
      "iteration: 200/200, cost_time: 135s, train_loss: 1.5853, train_acc: 0.4539, test_loss: 1.4315, test_acc: 0.4744\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "epoch 15/164:\n",
      "iteration: 200/200, cost_time: 135s, train_loss: 1.5679, train_acc: 0.4594, test_loss: 1.4178, test_acc: 0.4866\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "epoch 16/164:\n",
      "iteration: 200/200, cost_time: 136s, train_loss: 1.5523, train_acc: 0.4669, test_loss: 1.4029, test_acc: 0.4813\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "epoch 17/164:\n",
      "iteration: 200/200, cost_time: 136s, train_loss: 1.5377, train_acc: 0.4726, test_loss: 1.3929, test_acc: 0.4903\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "epoch 18/164:\n",
      "iteration: 200/200, cost_time: 135s, train_loss: 1.5239, train_acc: 0.4767, test_loss: 1.3825, test_acc: 0.4977\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "epoch 19/164:\n",
      "iteration: 200/200, cost_time: 135s, train_loss: 1.5179, train_acc: 0.4811, test_loss: 1.3715, test_acc: 0.5005\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "epoch 20/164:\n",
      "iteration: 200/200, cost_time: 138s, train_loss: 1.5047, train_acc: 0.4817, test_loss: 1.3636, test_acc: 0.5058\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "epoch 21/164:\n",
      "iteration: 200/200, cost_time: 135s, train_loss: 1.4923, train_acc: 0.4897, test_loss: 1.3493, test_acc: 0.5055\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "epoch 22/164:\n",
      "iteration: 200/200, cost_time: 136s, train_loss: 1.4834, train_acc: 0.4926, test_loss: 1.3403, test_acc: 0.5145\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "epoch 23/164:\n",
      "iteration: 200/200, cost_time: 135s, train_loss: 1.4690, train_acc: 0.4967, test_loss: 1.3359, test_acc: 0.5142\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "epoch 24/164:\n",
      "iteration: 200/200, cost_time: 137s, train_loss: 1.4594, train_acc: 0.5008, test_loss: 1.3280, test_acc: 0.5189\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "epoch 25/164:\n",
      "iteration: 200/200, cost_time: 138s, train_loss: 1.4548, train_acc: 0.5021, test_loss: 1.3209, test_acc: 0.5188\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "epoch 26/164:\n",
      "iteration: 200/200, cost_time: 137s, train_loss: 1.4434, train_acc: 0.5071, test_loss: 1.3126, test_acc: 0.5255\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "epoch 27/164:\n",
      "iteration: 200/200, cost_time: 137s, train_loss: 1.4411, train_acc: 0.5074, test_loss: 1.3092, test_acc: 0.5235\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "epoch 28/164:\n",
      "iteration: 200/200, cost_time: 136s, train_loss: 1.4329, train_acc: 0.5098, test_loss: 1.3029, test_acc: 0.5303\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "epoch 29/164:\n",
      "iteration: 200/200, cost_time: 138s, train_loss: 1.4224, train_acc: 0.5124, test_loss: 1.2907, test_acc: 0.5370\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "epoch 30/164:\n",
      "iteration: 200/200, cost_time: 136s, train_loss: 1.4224, train_acc: 0.5161, test_loss: 1.2875, test_acc: 0.5328\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "epoch 31/164:\n",
      "iteration: 200/200, cost_time: 137s, train_loss: 1.4073, train_acc: 0.5193, test_loss: 1.2877, test_acc: 0.5345\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "epoch 32/164:\n",
      "iteration: 200/200, cost_time: 136s, train_loss: 1.4020, train_acc: 0.5215, test_loss: 1.2847, test_acc: 0.5341\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "epoch 33/164:\n",
      "iteration: 200/200, cost_time: 137s, train_loss: 1.4022, train_acc: 0.5214, test_loss: 1.2703, test_acc: 0.5417\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "epoch 34/164:\n",
      "iteration: 200/200, cost_time: 144s, train_loss: 1.3866, train_acc: 0.5301, test_loss: 1.2693, test_acc: 0.5442\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "epoch 35/164:\n",
      "iteration: 200/200, cost_time: 151s, train_loss: 1.3915, train_acc: 0.5269, test_loss: 1.2632, test_acc: 0.5469\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "epoch 36/164:\n",
      "iteration: 200/200, cost_time: 144s, train_loss: 1.3789, train_acc: 0.5293, test_loss: 1.2559, test_acc: 0.5470\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "epoch 37/164:\n",
      "iteration: 200/200, cost_time: 138s, train_loss: 1.3781, train_acc: 0.5307, test_loss: 1.2550, test_acc: 0.5482\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "epoch 38/164:\n",
      "iteration: 200/200, cost_time: 135s, train_loss: 1.3751, train_acc: 0.5318, test_loss: 1.2459, test_acc: 0.5492\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "epoch 39/164:\n",
      "iteration: 200/200, cost_time: 142s, train_loss: 1.3640, train_acc: 0.5370, test_loss: 1.2405, test_acc: 0.5529\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "epoch 40/164:\n",
      "iteration: 200/200, cost_time: 141s, train_loss: 1.3620, train_acc: 0.5377, test_loss: 1.2366, test_acc: 0.5543\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "epoch 41/164:\n",
      "iteration: 200/200, cost_time: 142s, train_loss: 1.3558, train_acc: 0.5385, test_loss: 1.2331, test_acc: 0.5572\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "epoch 42/164:\n",
      "iteration: 200/200, cost_time: 144s, train_loss: 1.3528, train_acc: 0.5397, test_loss: 1.2331, test_acc: 0.5624\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "epoch 43/164:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 200/200, cost_time: 145s, train_loss: 1.3494, train_acc: 0.5404, test_loss: 1.2247, test_acc: 0.5625\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "epoch 44/164:\n",
      "iteration: 200/200, cost_time: 139s, train_loss: 1.3443, train_acc: 0.5413, test_loss: 1.2223, test_acc: 0.5613\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "epoch 45/164:\n",
      "iteration: 200/200, cost_time: 140s, train_loss: 1.3398, train_acc: 0.5457, test_loss: 1.2120, test_acc: 0.5649\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "epoch 46/164:\n",
      "iteration: 200/200, cost_time: 141s, train_loss: 1.3393, train_acc: 0.5450, test_loss: 1.2180, test_acc: 0.5630\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "epoch 47/164:\n",
      "iteration: 200/200, cost_time: 139s, train_loss: 1.3361, train_acc: 0.5472, test_loss: 1.2113, test_acc: 0.5700\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "epoch 48/164:\n",
      "iteration: 200/200, cost_time: 140s, train_loss: 1.3296, train_acc: 0.5463, test_loss: 1.2076, test_acc: 0.5692\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "epoch 49/164:\n",
      "iteration: 200/200, cost_time: 149s, train_loss: 1.3261, train_acc: 0.5506, test_loss: 1.2059, test_acc: 0.5695\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "epoch 50/164:\n",
      "iteration: 200/200, cost_time: 150s, train_loss: 1.3185, train_acc: 0.5506, test_loss: 1.1993, test_acc: 0.5746\n",
      "[0.0023148148, 0.0035264757, 0.003797743, 0.0040486655, 0.0038248699, 0.0039011638, 0.0038079156, 0.0038570827, 0.0039545693, 0.0039342246, 0.003909217, 0.0037795173, 0.0038765802, 0.003888872, 0.0039248997, 0.0039541456, 0.0037876368, 0.0038760304, 0.003515625]\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "\n",
      "epoch 51/164:\n",
      "iteration: 200/200, cost_time: 161s, train_loss: 1.3202, train_acc: 0.5500, test_loss: 1.1924, test_acc: 0.5721\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "\n",
      "epoch 52/164:\n",
      "iteration: 200/200, cost_time: 157s, train_loss: 1.3143, train_acc: 0.5545, test_loss: 1.1920, test_acc: 0.5722\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "\n",
      "epoch 53/164:\n",
      "iteration: 200/200, cost_time: 155s, train_loss: 1.3156, train_acc: 0.5551, test_loss: 1.1886, test_acc: 0.5737\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "\n",
      "epoch 54/164:\n",
      "iteration: 200/200, cost_time: 151s, train_loss: 1.3089, train_acc: 0.5532, test_loss: 1.1868, test_acc: 0.5742\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "\n",
      "epoch 55/164:\n",
      "iteration: 200/200, cost_time: 153s, train_loss: 1.3058, train_acc: 0.5583, test_loss: 1.1859, test_acc: 0.5771\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "\n",
      "epoch 56/164:\n",
      "iteration: 200/200, cost_time: 151s, train_loss: 1.3062, train_acc: 0.5569, test_loss: 1.1837, test_acc: 0.5748\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "\n",
      "epoch 57/164:\n",
      "iteration: 200/200, cost_time: 150s, train_loss: 1.3005, train_acc: 0.5574, test_loss: 1.1833, test_acc: 0.5749\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "\n",
      "epoch 58/164:\n",
      "iteration: 200/200, cost_time: 160s, train_loss: 1.3022, train_acc: 0.5578, test_loss: 1.1790, test_acc: 0.5749\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "\n",
      "epoch 59/164:\n",
      "iteration: 200/200, cost_time: 160s, train_loss: 1.2881, train_acc: 0.5609, test_loss: 1.1756, test_acc: 0.5806\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "\n",
      "epoch 60/164:\n",
      "iteration: 200/200, cost_time: 156s, train_loss: 1.2918, train_acc: 0.5604, test_loss: 1.1746, test_acc: 0.5807\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "\n",
      "epoch 61/164:\n",
      "iteration: 200/200, cost_time: 154s, train_loss: 1.2942, train_acc: 0.5602, test_loss: 1.1735, test_acc: 0.5781\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "\n",
      "epoch 62/164:\n",
      "iteration: 200/200, cost_time: 152s, train_loss: 1.2877, train_acc: 0.5644, test_loss: 1.1704, test_acc: 0.5822\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "\n",
      "epoch 63/164:\n",
      "iteration: 200/200, cost_time: 152s, train_loss: 1.2894, train_acc: 0.5604, test_loss: 1.1703, test_acc: 0.5807\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "\n",
      "epoch 64/164:\n",
      "iteration: 200/200, cost_time: 153s, train_loss: 1.2824, train_acc: 0.5644, test_loss: 1.1689, test_acc: 0.5812\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "\n",
      "epoch 65/164:\n",
      "iteration: 200/200, cost_time: 150s, train_loss: 1.2806, train_acc: 0.5677, test_loss: 1.1622, test_acc: 0.5825\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "\n",
      "epoch 66/164:\n",
      "iteration: 200/200, cost_time: 152s, train_loss: 1.2759, train_acc: 0.5664, test_loss: 1.1644, test_acc: 0.5829\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "\n",
      "epoch 67/164:\n",
      "iteration: 200/200, cost_time: 156s, train_loss: 1.2795, train_acc: 0.5617, test_loss: 1.1587, test_acc: 0.5878\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "\n",
      "epoch 68/164:\n",
      "iteration: 200/200, cost_time: 152s, train_loss: 1.2716, train_acc: 0.5705, test_loss: 1.1553, test_acc: 0.5871\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "\n",
      "epoch 69/164:\n",
      "iteration: 200/200, cost_time: 159s, train_loss: 1.2766, train_acc: 0.5666, test_loss: 1.1569, test_acc: 0.5875\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "\n",
      "epoch 70/164:\n",
      "iteration: 200/200, cost_time: 161s, train_loss: 1.2682, train_acc: 0.5697, test_loss: 1.1548, test_acc: 0.5874\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "\n",
      "epoch 71/164:\n",
      "iteration: 200/200, cost_time: 160s, train_loss: 1.2722, train_acc: 0.5692, test_loss: 1.1541, test_acc: 0.5870\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "\n",
      "epoch 72/164:\n",
      "iteration: 200/200, cost_time: 151s, train_loss: 1.2668, train_acc: 0.5712, test_loss: 1.1524, test_acc: 0.5903\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "\n",
      "epoch 73/164:\n",
      "iteration: 200/200, cost_time: 151s, train_loss: 1.2669, train_acc: 0.5696, test_loss: 1.1514, test_acc: 0.5915\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "\n",
      "epoch 74/164:\n",
      "iteration: 200/200, cost_time: 151s, train_loss: 1.2608, train_acc: 0.5745, test_loss: 1.1509, test_acc: 0.5855\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "\n",
      "epoch 75/164:\n",
      "iteration: 200/200, cost_time: 151s, train_loss: 1.2590, train_acc: 0.5710, test_loss: 1.1449, test_acc: 0.5916\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "\n",
      "epoch 76/164:\n",
      "iteration: 200/200, cost_time: 152s, train_loss: 1.2639, train_acc: 0.5725, test_loss: 1.1451, test_acc: 0.5947\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "\n",
      "epoch 77/164:\n",
      "iteration: 200/200, cost_time: 156s, train_loss: 1.2599, train_acc: 0.5706, test_loss: 1.1447, test_acc: 0.5947\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "\n",
      "epoch 78/164:\n",
      "iteration: 200/200, cost_time: 161s, train_loss: 1.2545, train_acc: 0.5770, test_loss: 1.1438, test_acc: 0.5919\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "\n",
      "epoch 79/164:\n",
      "iteration: 200/200, cost_time: 162s, train_loss: 1.2569, train_acc: 0.5745, test_loss: 1.1421, test_acc: 0.5915\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "\n",
      "epoch 80/164:\n",
      "iteration: 200/200, cost_time: 159s, train_loss: 1.2531, train_acc: 0.5743, test_loss: 1.1394, test_acc: 0.5939\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "\n",
      "epoch 81/164:\n",
      "iteration: 200/200, cost_time: 152s, train_loss: 1.2509, train_acc: 0.5780, test_loss: 1.1368, test_acc: 0.5942\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "\n",
      "epoch 82/164:\n",
      "iteration: 200/200, cost_time: 151s, train_loss: 1.2515, train_acc: 0.5746, test_loss: 1.1425, test_acc: 0.5903\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "\n",
      "epoch 83/164:\n",
      "iteration: 200/200, cost_time: 155s, train_loss: 1.2453, train_acc: 0.5751, test_loss: 1.1374, test_acc: 0.5921\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "\n",
      "epoch 84/164:\n",
      "iteration: 200/200, cost_time: 151s, train_loss: 1.2466, train_acc: 0.5779, test_loss: 1.1364, test_acc: 0.5949\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "\n",
      "epoch 85/164:\n",
      "iteration: 200/200, cost_time: 151s, train_loss: 1.2422, train_acc: 0.5786, test_loss: 1.1363, test_acc: 0.5925\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "\n",
      "epoch 86/164:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 200/200, cost_time: 158s, train_loss: 1.2442, train_acc: 0.5786, test_loss: 1.1347, test_acc: 0.5921\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "\n",
      "epoch 87/164:\n",
      "iteration: 200/200, cost_time: 161s, train_loss: 1.2444, train_acc: 0.5774, test_loss: 1.1309, test_acc: 0.5966\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "\n",
      "epoch 88/164:\n",
      "iteration: 200/200, cost_time: 161s, train_loss: 1.2438, train_acc: 0.5767, test_loss: 1.1316, test_acc: 0.5973\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "\n",
      "epoch 89/164:\n",
      "iteration: 200/200, cost_time: 157s, train_loss: 1.2414, train_acc: 0.5780, test_loss: 1.1311, test_acc: 0.5935\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "\n",
      "epoch 90/164:\n",
      "iteration: 200/200, cost_time: 154s, train_loss: 1.2405, train_acc: 0.5821, test_loss: 1.1297, test_acc: 0.5933\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "\n",
      "epoch 91/164:\n",
      "iteration: 200/200, cost_time: 152s, train_loss: 1.2375, train_acc: 0.5821, test_loss: 1.1284, test_acc: 0.5932\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "\n",
      "epoch 92/164:\n",
      "iteration: 200/200, cost_time: 155s, train_loss: 1.2362, train_acc: 0.5817, test_loss: 1.1285, test_acc: 0.5960\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "\n",
      "epoch 93/164:\n",
      "iteration: 200/200, cost_time: 151s, train_loss: 1.2362, train_acc: 0.5831, test_loss: 1.1233, test_acc: 0.5981\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "\n",
      "epoch 94/164:\n",
      "iteration: 200/200, cost_time: 150s, train_loss: 1.2375, train_acc: 0.5792, test_loss: 1.1227, test_acc: 0.5992\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "\n",
      "epoch 95/164:\n",
      "iteration: 200/200, cost_time: 157s, train_loss: 1.2338, train_acc: 0.5805, test_loss: 1.1225, test_acc: 0.5999\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "\n",
      "epoch 96/164:\n",
      "iteration: 200/200, cost_time: 161s, train_loss: 1.2311, train_acc: 0.5854, test_loss: 1.1185, test_acc: 0.6007\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "\n",
      "epoch 97/164:\n",
      "iteration: 200/200, cost_time: 161s, train_loss: 1.2345, train_acc: 0.5824, test_loss: 1.1196, test_acc: 0.5964\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "\n",
      "epoch 98/164:\n",
      "iteration: 200/200, cost_time: 157s, train_loss: 1.2272, train_acc: 0.5825, test_loss: 1.1188, test_acc: 0.6004\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "\n",
      "epoch 99/164:\n",
      "iteration: 200/200, cost_time: 154s, train_loss: 1.2305, train_acc: 0.5851, test_loss: 1.1156, test_acc: 0.6004\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "\n",
      "epoch 100/164:\n",
      "iteration: 200/200, cost_time: 150s, train_loss: 1.2272, train_acc: 0.5828, test_loss: 1.1183, test_acc: 0.6028\n",
      "[0.0034722222, 0.004692925, 0.0049913195, 0.004964193, 0.004479302, 0.004413181, 0.0044725207, 0.0042131213, 0.004146152, 0.0041864184, 0.0041088527, 0.0040825736, 0.003999498, 0.0043258667, 0.0042915344, 0.0042682225, 0.0046892166, 0.0042178035, 0.004370117]\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "\n",
      "epoch 101/164:\n",
      "iteration: 200/200, cost_time: 153s, train_loss: 1.2286, train_acc: 0.5873, test_loss: 1.1131, test_acc: 0.6027\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "\n",
      "epoch 102/164:\n",
      "iteration: 200/200, cost_time: 151s, train_loss: 1.2294, train_acc: 0.5821, test_loss: 1.1153, test_acc: 0.6021\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "\n",
      "epoch 103/164:\n",
      "iteration: 200/200, cost_time: 150s, train_loss: 1.2290, train_acc: 0.5829, test_loss: 1.1124, test_acc: 0.6029\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "\n",
      "epoch 104/164:\n",
      "iteration: 200/200, cost_time: 155s, train_loss: 1.2235, train_acc: 0.5868, test_loss: 1.1138, test_acc: 0.6017\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "\n",
      "epoch 105/164:\n",
      "iteration: 200/200, cost_time: 160s, train_loss: 1.2255, train_acc: 0.5832, test_loss: 1.1148, test_acc: 0.6003\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "\n",
      "epoch 106/164:\n",
      "iteration: 200/200, cost_time: 159s, train_loss: 1.2285, train_acc: 0.5843, test_loss: 1.1092, test_acc: 0.6048\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "\n",
      "epoch 107/164:\n",
      "iteration: 200/200, cost_time: 156s, train_loss: 1.2218, train_acc: 0.5861, test_loss: 1.1098, test_acc: 0.6006\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "\n",
      "epoch 108/164:\n",
      "iteration: 200/200, cost_time: 154s, train_loss: 1.2214, train_acc: 0.5831, test_loss: 1.1096, test_acc: 0.6045\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "\n",
      "epoch 109/164:\n",
      "iteration: 200/200, cost_time: 152s, train_loss: 1.2224, train_acc: 0.5853, test_loss: 1.1112, test_acc: 0.6086\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "\n",
      "epoch 110/164:\n",
      "iteration: 200/200, cost_time: 153s, train_loss: 1.2214, train_acc: 0.5875, test_loss: 1.1094, test_acc: 0.6086\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "\n",
      "epoch 111/164:\n",
      "iteration: 200/200, cost_time: 153s, train_loss: 1.2178, train_acc: 0.5856, test_loss: 1.1077, test_acc: 0.6053\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "\n",
      "epoch 112/164:\n",
      "iteration: 200/200, cost_time: 150s, train_loss: 1.2150, train_acc: 0.5892, test_loss: 1.1100, test_acc: 0.6050\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "\n",
      "epoch 113/164:\n",
      "iteration: 200/200, cost_time: 155s, train_loss: 1.2157, train_acc: 0.5893, test_loss: 1.1042, test_acc: 0.6046\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "\n",
      "epoch 114/164:\n",
      "iteration: 200/200, cost_time: 155s, train_loss: 1.2145, train_acc: 0.5882, test_loss: 1.1091, test_acc: 0.6063\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "\n",
      "epoch 115/164:\n",
      "iteration: 200/200, cost_time: 156s, train_loss: 1.2123, train_acc: 0.5903, test_loss: 1.1060, test_acc: 0.6040\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "\n",
      "epoch 116/164:\n",
      "iteration: 200/200, cost_time: 154s, train_loss: 1.2149, train_acc: 0.5896, test_loss: 1.1034, test_acc: 0.6040\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "\n",
      "epoch 117/164:\n",
      "iteration: 200/200, cost_time: 159s, train_loss: 1.2108, train_acc: 0.5925, test_loss: 1.1015, test_acc: 0.6104\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "\n",
      "epoch 118/164:\n",
      "iteration: 200/200, cost_time: 152s, train_loss: 1.2103, train_acc: 0.5901, test_loss: 1.1013, test_acc: 0.6063\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "\n",
      "epoch 119/164:\n",
      "iteration: 200/200, cost_time: 152s, train_loss: 1.2121, train_acc: 0.5901, test_loss: 1.0975, test_acc: 0.6101\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "\n",
      "epoch 120/164:\n",
      "iteration: 200/200, cost_time: 150s, train_loss: 1.2103, train_acc: 0.5892, test_loss: 1.1036, test_acc: 0.6099\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "\n",
      "epoch 121/164:\n",
      "iteration: 200/200, cost_time: 152s, train_loss: 1.2063, train_acc: 0.5908, test_loss: 1.0996, test_acc: 0.6091\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "\n",
      "epoch 122/164:\n",
      "iteration: 200/200, cost_time: 153s, train_loss: 1.2089, train_acc: 0.5927, test_loss: 1.0983, test_acc: 0.6086\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "\n",
      "epoch 123/164:\n",
      "iteration: 200/200, cost_time: 152s, train_loss: 1.2070, train_acc: 0.5898, test_loss: 1.0985, test_acc: 0.6109\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "\n",
      "epoch 124/164:\n",
      "iteration: 200/200, cost_time: 158s, train_loss: 1.2074, train_acc: 0.5922, test_loss: 1.0955, test_acc: 0.6103\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "\n",
      "epoch 125/164:\n",
      "iteration: 200/200, cost_time: 161s, train_loss: 1.2048, train_acc: 0.5936, test_loss: 1.0994, test_acc: 0.6101\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "\n",
      "epoch 126/164:\n",
      "iteration: 200/200, cost_time: 161s, train_loss: 1.2019, train_acc: 0.5943, test_loss: 1.0992, test_acc: 0.6110\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "\n",
      "epoch 127/164:\n",
      "iteration: 200/200, cost_time: 153s, train_loss: 1.2072, train_acc: 0.5903, test_loss: 1.0999, test_acc: 0.6134\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "\n",
      "epoch 128/164:\n",
      "iteration: 200/200, cost_time: 152s, train_loss: 1.2019, train_acc: 0.5956, test_loss: 1.0970, test_acc: 0.6088\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "\n",
      "epoch 129/164:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 200/200, cost_time: 151s, train_loss: 1.2034, train_acc: 0.5900, test_loss: 1.0931, test_acc: 0.6100\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "\n",
      "epoch 130/164:\n",
      "iteration: 200/200, cost_time: 151s, train_loss: 1.2020, train_acc: 0.5946, test_loss: 1.0951, test_acc: 0.6103\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "\n",
      "epoch 131/164:\n",
      "iteration: 200/200, cost_time: 154s, train_loss: 1.2032, train_acc: 0.5933, test_loss: 1.0952, test_acc: 0.6101\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "\n",
      "epoch 132/164:\n",
      "iteration: 200/200, cost_time: 153s, train_loss: 1.2017, train_acc: 0.5941, test_loss: 1.0959, test_acc: 0.6108\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "\n",
      "epoch 133/164:\n",
      "iteration: 200/200, cost_time: 158s, train_loss: 1.1991, train_acc: 0.5933, test_loss: 1.0932, test_acc: 0.6125\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "\n",
      "epoch 134/164:\n",
      "iteration: 200/200, cost_time: 154s, train_loss: 1.2011, train_acc: 0.5928, test_loss: 1.0937, test_acc: 0.6123\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "\n",
      "epoch 135/164:\n",
      "iteration: 200/200, cost_time: 151s, train_loss: 1.2027, train_acc: 0.5920, test_loss: 1.0923, test_acc: 0.6115\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "\n",
      "epoch 136/164:\n",
      "iteration: 200/200, cost_time: 152s, train_loss: 1.1944, train_acc: 0.5939, test_loss: 1.0950, test_acc: 0.6110\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "\n",
      "epoch 137/164:\n",
      "iteration: 200/200, cost_time: 158s, train_loss: 1.1961, train_acc: 0.5978, test_loss: 1.0895, test_acc: 0.6122\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "\n",
      "epoch 138/164:\n",
      "iteration: 200/200, cost_time: 150s, train_loss: 1.1966, train_acc: 0.5951, test_loss: 1.0911, test_acc: 0.6108\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "\n",
      "epoch 139/164:\n",
      "iteration: 200/200, cost_time: 154s, train_loss: 1.2015, train_acc: 0.5940, test_loss: 1.0916, test_acc: 0.6108\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "\n",
      "epoch 140/164:\n",
      "iteration: 200/200, cost_time: 150s, train_loss: 1.1967, train_acc: 0.5956, test_loss: 1.0883, test_acc: 0.6142\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "\n",
      "epoch 141/164:\n",
      "iteration: 200/200, cost_time: 161s, train_loss: 1.1970, train_acc: 0.5956, test_loss: 1.0859, test_acc: 0.6156\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "\n",
      "epoch 142/164:\n",
      "iteration: 200/200, cost_time: 155s, train_loss: 1.1965, train_acc: 0.5983, test_loss: 1.0874, test_acc: 0.6134\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "\n",
      "epoch 143/164:\n",
      "iteration: 200/200, cost_time: 153s, train_loss: 1.2011, train_acc: 0.5950, test_loss: 1.0890, test_acc: 0.6124\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "\n",
      "epoch 144/164:\n",
      "iteration: 200/200, cost_time: 149s, train_loss: 1.1922, train_acc: 0.5964, test_loss: 1.0878, test_acc: 0.6124\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "\n",
      "epoch 145/164:\n",
      "iteration: 200/200, cost_time: 157s, train_loss: 1.1932, train_acc: 0.5988, test_loss: 1.0860, test_acc: 0.6165\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "\n",
      "epoch 146/164:\n",
      "iteration: 200/200, cost_time: 156s, train_loss: 1.1944, train_acc: 0.5953, test_loss: 1.0881, test_acc: 0.6131\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "\n",
      "epoch 147/164:\n",
      "iteration: 200/200, cost_time: 152s, train_loss: 1.1923, train_acc: 0.5991, test_loss: 1.0863, test_acc: 0.6140\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "\n",
      "epoch 148/164:\n",
      "iteration: 200/200, cost_time: 150s, train_loss: 1.1969, train_acc: 0.5955, test_loss: 1.0892, test_acc: 0.6129\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "\n",
      "epoch 149/164:\n",
      "iteration: 200/200, cost_time: 156s, train_loss: 1.1925, train_acc: 0.5972, test_loss: 1.0847, test_acc: 0.6145\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "\n",
      "epoch 150/164:\n",
      "iteration: 200/200, cost_time: 157s, train_loss: 1.1952, train_acc: 0.5977, test_loss: 1.0851, test_acc: 0.6130\n",
      "[0.0034722222, 0.004638672, 0.0049913195, 0.0046657985, 0.0045776367, 0.004582723, 0.004740397, 0.0044962564, 0.004307217, 0.0043148464, 0.0042474535, 0.0042334665, 0.0041715833, 0.004539066, 0.0044025844, 0.0044233534, 0.0048485994, 0.00447315, 0.004272461]\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "\n",
      "epoch 151/164:\n",
      "iteration: 200/200, cost_time: 150s, train_loss: 1.1919, train_acc: 0.5978, test_loss: 1.0887, test_acc: 0.6097\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "\n",
      "epoch 152/164:\n",
      "iteration: 200/200, cost_time: 151s, train_loss: 1.1899, train_acc: 0.5979, test_loss: 1.0811, test_acc: 0.6154\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "\n",
      "epoch 153/164:\n",
      "iteration: 200/200, cost_time: 157s, train_loss: 1.1871, train_acc: 0.5971, test_loss: 1.0816, test_acc: 0.6179\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "\n",
      "epoch 154/164:\n",
      "iteration: 200/200, cost_time: 155s, train_loss: 1.1870, train_acc: 0.6010, test_loss: 1.0848, test_acc: 0.6125\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "\n",
      "epoch 155/164:\n",
      "iteration: 200/200, cost_time: 150s, train_loss: 1.1855, train_acc: 0.6003, test_loss: 1.0794, test_acc: 0.6136\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "\n",
      "epoch 156/164:\n",
      "iteration: 200/200, cost_time: 152s, train_loss: 1.1875, train_acc: 0.6000, test_loss: 1.0780, test_acc: 0.6131\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "\n",
      "epoch 157/164:\n",
      "iteration: 200/200, cost_time: 156s, train_loss: 1.1862, train_acc: 0.5999, test_loss: 1.0780, test_acc: 0.6172\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "\n",
      "epoch 158/164:\n",
      "iteration: 200/200, cost_time: 154s, train_loss: 1.1872, train_acc: 0.5990, test_loss: 1.0780, test_acc: 0.6175\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "\n",
      "epoch 159/164:\n",
      "iteration: 200/200, cost_time: 152s, train_loss: 1.1874, train_acc: 0.5980, test_loss: 1.0806, test_acc: 0.6151\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "\n",
      "epoch 160/164:\n",
      "iteration: 200/200, cost_time: 152s, train_loss: 1.1862, train_acc: 0.5978, test_loss: 1.0818, test_acc: 0.6129\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "\n",
      "epoch 161/164:\n",
      "iteration: 200/200, cost_time: 155s, train_loss: 1.1867, train_acc: 0.5993, test_loss: 1.0763, test_acc: 0.6178\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "\n",
      "epoch 162/164:\n",
      "iteration: 200/200, cost_time: 152s, train_loss: 1.1855, train_acc: 0.5978, test_loss: 1.0857, test_acc: 0.6146\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "\n",
      "epoch 163/164:\n",
      "iteration: 200/200, cost_time: 151s, train_loss: 1.1859, train_acc: 0.5998, test_loss: 1.0792, test_acc: 0.6159\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "\n",
      "epoch 164/164:\n",
      "iteration: 200/200, cost_time: 151s, train_loss: 1.1838, train_acc: 0.6000, test_loss: 1.0801, test_acc: 0.6188\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "Model saved in file: ./model/\n",
      "[[0.0017361111, 0.0011664496, 0.0011800131, 0.0009426541, 0.00088500977, 0.00023227268, 0.00029500326, 0.00026787652, 0.00027126737, 1.1020236e-05, 1.1867947e-05, 1.1444092e-05, 1.0596381e-05, 1.1444092e-05, 1.0596381e-05, 1.1020236e-05, 0.00037622452, 2.8431416e-05, 7.324219e-05], [0.0023148148, 0.0034179688, 0.0035807292, 0.003370497, 0.0034823949, 0.0035536024, 0.0036044652, 0.0036265056, 0.003610399, 0.0034637451, 0.003455268, 0.0033713446, 0.0033925374, 0.0032747057, 0.0032950507, 0.0032229954, 0.0031930208, 0.0031125546, 0.003515625], [0.0017361111, 0.0032552083, 0.0034044054, 0.0035468207, 0.003797743, 0.004023234, 0.003994412, 0.0039232043, 0.0040571424, 0.003996531, 0.004023234, 0.003991869, 0.0039702523, 0.0041389465, 0.0040601096, 0.004090627, 0.0043144226, 0.0042925477, 0.0037841797], [0.0046296297, 0.0033094618, 0.0034857856, 0.0034857856, 0.0036587184, 0.003821479, 0.0038180882, 0.003902859, 0.0038960774, 0.003921509, 0.0038168165, 0.003798167, 0.0037943523, 0.0038965014, 0.0038808186, 0.0038549635, 0.004142642, 0.0039461255, 0.0032958984], [0.0023148148, 0.0033637153, 0.0032145183, 0.0038994683, 0.0037027996, 0.0038604736, 0.0038706462, 0.0040096706, 0.0038494533, 0.0037549336, 0.0036883885, 0.0037172106, 0.0037032233, 0.003584544, 0.0036349827, 0.0035273235, 0.0035054684, 0.0035117865, 0.0037597655], [0.0034722222, 0.0033094618, 0.0038926867, 0.003587511, 0.0037672254, 0.0038876003, 0.0038757324, 0.0038706462, 0.0039978027, 0.0040660435, 0.0040177237, 0.0040100943, 0.004040188, 0.0041537816, 0.0042177835, 0.004134708, 0.004358053, 0.0044419765, 0.003930664], [0.0063657407, 0.0038519965, 0.0039333766, 0.0037163629, 0.0039605033, 0.003975762, 0.0037943523, 0.0038977729, 0.003821479, 0.0037968953, 0.0038846333, 0.0039041308, 0.0038736132, 0.0040151807, 0.0040071276, 0.004007975, 0.0040609837, 0.004027605, 0.0040283203], [0.0028935184, 0.0028211805, 0.0036078559, 0.003797743, 0.0037095812, 0.0038926867, 0.0038367377, 0.003963894, 0.0038019817, 0.0037502712, 0.003824446, 0.0038202074, 0.0037608678, 0.0037384033, 0.003613366, 0.003721449, 0.003637433, 0.003563702, 0.0037841797], [0.0011574074, 0.0034450954, 0.0038384332, 0.0036824543, 0.003862169, 0.0039232043, 0.003938463, 0.0040978324, 0.0039588083, 0.00394143, 0.0039689806, 0.003943973, 0.0040740967, 0.004200829, 0.004131741, 0.004216088, 0.0043076277, 0.0044662356, 0.003515625], [0.0034722222, 0.0035264757, 0.0037706164, 0.0036756727, 0.003648546, 0.0038977729, 0.0038655598, 0.0037384033, 0.0039037068, 0.0038846333, 0.0038261414, 0.0038664076, 0.0038617451, 0.004027473, 0.0040198434, 0.0039978027, 0.0040402412, 0.004038751, 0.0037109375], [0.0023148148, 0.0033908421, 0.0037434895, 0.003411187, 0.0034993489, 0.0037926568, 0.0037875706, 0.0039198133, 0.0038867528, 0.0037952, 0.0038240221, 0.003803677, 0.0037549336, 0.0036697388, 0.003730774, 0.003715939, 0.003682971, 0.0036894083, 0.003979492], [0.0005787037, 0.0039605033, 0.0039198133, 0.0039876304, 0.0038486058, 0.003902859, 0.0038689508, 0.003909641, 0.003882514, 0.0039193896, 0.0038846333, 0.003970676, 0.0039011638, 0.0041571725, 0.0040838453, 0.004064348, 0.004170418, 0.0042983294, 0.0037841797], [0.0017361111, 0.0037163629, 0.0037841797, 0.0037706164, 0.003696018, 0.0040096706, 0.003953722, 0.0040334067, 0.003885905, 0.0037930806, 0.0038138495, 0.0038926867, 0.0038587782, 0.003953722, 0.0039761863, 0.0040134853, 0.0038334131, 0.003917873, 0.0040283203], [0.0017361111, 0.0039876304, 0.0035807292, 0.003689236, 0.0037400988, 0.0037282307, 0.0038452148, 0.003826565, 0.0038833618, 0.00387107, 0.0038274128, 0.003756629, 0.00383589, 0.003812578, 0.0038176642, 0.0038320753, 0.0038491488, 0.0038539767, 0.003540039], [0.0011574074, 0.0039333766, 0.0039333766, 0.003885905, 0.0038994683, 0.003957113, 0.0039248997, 0.0039401585, 0.003888448, 0.0039541456, 0.0039427015, 0.0038892957, 0.004002465, 0.0041088527, 0.003973219, 0.0039668614, 0.0041697025, 0.004173398, 0.0037841797], [0.0034722222, 0.0034450954, 0.004055447, 0.003668891, 0.003774007, 0.0038876003, 0.003829956, 0.003853692, 0.003862169, 0.003739251, 0.0037909613, 0.0037460327, 0.0037621392, 0.0037765503, 0.003815121, 0.00380071, 0.003649354, 0.0037564635, 0.004174805], [0.004050926, 0.0038519965, 0.0036078559, 0.003668891, 0.004113091, 0.0037723118, 0.0038791234, 0.0039401585, 0.0038808186, 0.0039253235, 0.0038842096, 0.0038820903, 0.0039431253, 0.003908793, 0.003993988, 0.0039371913, 0.0041000843, 0.0040789843, 0.0037597655], [0.004050926, 0.0031195746, 0.00390625, 0.0038655598, 0.003994412, 0.0039282907, 0.0038994683, 0.003914727, 0.0039274427, 0.003850301, 0.0039164224, 0.003937615, 0.0039287144, 0.0039681327, 0.0040397644, 0.0040567187, 0.0041433573, 0.0040838122, 0.0038330078], [0.0046296297, 0.0039333766, 0.0038519965, 0.0038384332, 0.0039333766, 0.0036655003, 0.003668891, 0.0038452148, 0.003777398, 0.0037697686, 0.0037235683, 0.0037434895, 0.0037896899, 0.0036900837, 0.0037680732, 0.0036561755, 0.0035289526, 0.0036414862, 0.0038330078], [0.0017361111, 0.0034450954, 0.0039605033, 0.0039333766, 0.0038384332, 0.0038943822, 0.003941854, 0.003963894, 0.0038316515, 0.0039299857, 0.003908369, 0.003956265, 0.003969828, 0.003997379, 0.0040329825, 0.003984663, 0.004245043, 0.00426054, 0.0040283203], [0.0052083335, 0.004340278, 0.003540039, 0.003974067, 0.00390625, 0.003985935, 0.0039689806, 0.003753662, 0.0038689508, 0.003932529, 0.0038312275, 0.003910912, 0.003950331, 0.004134708, 0.004093594, 0.0040541757, 0.0042454004, 0.0040765405, 0.003857422], [0.0034722222, 0.003797743, 0.0033908421, 0.0038384332, 0.0037943523, 0.0038197835, 0.003801134, 0.0037706164, 0.003821479, 0.0037807887, 0.0038235981, 0.0037591723, 0.0037430658, 0.0036824543, 0.0036557515, 0.003640069, 0.003523469, 0.0036360621, 0.0036376952], [0.0023148148, 0.0036078559, 0.0040283203, 0.003757053, 0.0037875706, 0.0039723716, 0.003882514, 0.0038587782, 0.0038579304, 0.003909217, 0.003956265, 0.0039308337, 0.003956265, 0.004078759, 0.0040694345, 0.0041296217, 0.0042597055, 0.004295349, 0.0037109375], [0.0034722222, 0.0036349827, 0.003648546, 0.0037706164, 0.0038655598, 0.0038045247, 0.0038231744, 0.003935072, 0.0039460924, 0.0039024353, 0.0039787292, 0.0039070975, 0.0039431253, 0.004090203, 0.0041020713, 0.004037645, 0.004256487, 0.0040504336, 0.003979492], [0.004050926, 0.0038791234, 0.0039876304, 0.0038180882, 0.0037604438, 0.0038486058, 0.0037875706, 0.0038655598, 0.0038350422, 0.0037553576, 0.0037409465, 0.0038549635, 0.0037799412, 0.0036472743, 0.0036006505, 0.0036320155, 0.0036059618, 0.0036519766, 0.004370117], [0.0011574074, 0.004123264, 0.0040961374, 0.0038316515, 0.0039808485, 0.0037316217, 0.0039198133, 0.0038282606, 0.003915575, 0.0039113364, 0.0039159986, 0.0039507546, 0.004035102, 0.0041859946, 0.0041342843, 0.0042173597, 0.004257202, 0.004316032, 0.0043212892], [0.0023148148, 0.0033365886, 0.0035671657, 0.003974067, 0.004075792, 0.0038163927, 0.004007975, 0.003862169, 0.0038833618, 0.0038583544, 0.0038553874, 0.00383589, 0.0038867528, 0.004044851, 0.0041156346, 0.0040986803, 0.0040746927, 0.0039770007, 0.0037597655], [0.004050926, 0.004421658, 0.0034179688, 0.0041300454, 0.0039333766, 0.003841824, 0.0037672254, 0.0038706462, 0.0038943822, 0.003780365, 0.0038083394, 0.0037935046, 0.0037320454, 0.0037668017, 0.003724416, 0.0037172106, 0.0037117004, 0.003715396, 0.003491211], [0.0034722222, 0.0039876304, 0.0038248699, 0.004102919, 0.0039774575, 0.0039011638, 0.00406562, 0.003926595, 0.0040003457, 0.0039435495, 0.003996531, 0.0038833618, 0.003923628, 0.0041097007, 0.004121145, 0.004210154, 0.004243374, 0.0042964816, 0.003540039], [0.0034722222, 0.0034450954, 0.00394694, 0.0038791234, 0.003668891, 0.003991021, 0.003801134, 0.003850301, 0.0038833618, 0.0038354662, 0.003821903, 0.0038303798, 0.0038600499, 0.003985511, 0.0039079455, 0.0038926867, 0.0039463043, 0.0038831234, 0.003540039], [0.0023148148, 0.0032823351, 0.0032687718, 0.0036553277, 0.003909641, 0.0039232043, 0.003877428, 0.003950331, 0.0038655598, 0.0037672254, 0.0038405524, 0.0038337708, 0.0038668315, 0.003844791, 0.0038469103, 0.0037884181, 0.00391376, 0.003901422, 0.0038818358], [0.004050926, 0.0043674046, 0.0039198133, 0.0037841797, 0.004007975, 0.0038163927, 0.0038367377, 0.0039401585, 0.003921509, 0.003985087, 0.003996955, 0.0040105185, 0.003988478, 0.0040334067, 0.0041207206, 0.0040740967, 0.0042259693, 0.004189849, 0.0038818358], [0.0017361111, 0.0036349827, 0.0035942926, 0.0037163629, 0.0037061903, 0.0037672254, 0.0039045545, 0.0039689806, 0.003918966, 0.0038935344, 0.0038553874, 0.0037854512, 0.0038634406, 0.0037973193, 0.0038757324, 0.003877004, 0.003681302, 0.0038147569, 0.004199219], [0.004050926, 0.0039876304, 0.0036621094, 0.0038316515, 0.0039978027, 0.003841824, 0.003938463, 0.003801134, 0.003853692, 0.0039668614, 0.0038994683, 0.0038422479, 0.0038541157, 0.003900316, 0.0039321054, 0.0038630168, 0.0040193796, 0.004021108, 0.0036376952], [0.004050926, 0.0039605033, 0.0039333766, 0.003953722, 0.0039842394, 0.0039045545, 0.003921509, 0.003970676, 0.0038791234, 0.0039431253, 0.00395245, 0.003929562, 0.0039282907, 0.0041482714, 0.0040745204, 0.0041033425, 0.004269123, 0.0041118264, 0.0038330078], [0.0028935184, 0.0034179688, 0.003797743, 0.0037502712, 0.0037095812, 0.003918118, 0.0037892659, 0.003979153, 0.0038553874, 0.0038231744, 0.0037926568, 0.0038350422, 0.003841824, 0.0036760967, 0.0036985609, 0.0037731593, 0.0035647154, 0.003721714, 0.004272461], [0.0028935184, 0.0030653211, 0.0040961374, 0.0038994683, 0.0039299857, 0.003941854, 0.0040384927, 0.003809611, 0.0038901435, 0.0038676793, 0.0039066738, 0.0038486058, 0.0038939582, 0.004002889, 0.0040109423, 0.003955841, 0.0041468143, 0.004160166, 0.003515625], [0.0034722222, 0.0038791234, 0.0036756727, 0.004224989, 0.003967285, 0.003963894, 0.003938463, 0.003970676, 0.0038274128, 0.004026625, 0.003988478, 0.003951179, 0.003927019, 0.0040571424, 0.004119873, 0.004151238, 0.004316807, 0.0041009784, 0.0038330078], [0.0017361111, 0.0034722222, 0.0042860243, 0.0036282009, 0.0040317113, 0.0038909912, 0.0038435194, 0.0039248997, 0.0038791234, 0.0038240221, 0.0038592021, 0.003812154, 0.0037489997, 0.0036362542, 0.0036650763, 0.003707038, 0.0035725832, 0.0037100315, 0.0040527345], [0.0046296297, 0.0037163629, 0.0038113063, 0.00394694, 0.0039842394, 0.003833347, 0.004002889, 0.003974067, 0.0039494834, 0.0038871765, 0.0039486354, 0.0039579603, 0.003979153, 0.0040664673, 0.0040050084, 0.0040635, 0.0042357445, 0.0041837096, 0.0038330078], [0.004050926, 0.0036349827, 0.0039876304, 0.004014757, 0.00394694, 0.0040384927, 0.003801134, 0.0037960475, 0.0039664377, 0.0038990444, 0.0039774575, 0.0039507546, 0.003956265, 0.004239824, 0.004219479, 0.004207611, 0.004288316, 0.004087329, 0.0037597655], [0.0034722222, 0.0035264757, 0.0034857856, 0.0038926867, 0.0038180882, 0.0038197835, 0.0037587483, 0.0037163629, 0.003871494, 0.0038117303, 0.003856659, 0.0038028294, 0.0037608678, 0.0037409465, 0.003718906, 0.0037087335, 0.0036560297, 0.0037520528, 0.003979492], [0.0023148148, 0.0038791234, 0.003689236, 0.0040418836, 0.004011366, 0.003938463, 0.004035102, 0.0039520264, 0.0038664076, 0.0039494834, 0.003969828, 0.0039948355, 0.0040211147, 0.0040456983, 0.004108005, 0.0041419137, 0.0042675734, 0.004245162, 0.0044189454], [0.0017361111, 0.0036078559, 0.0037299262, 0.003967285, 0.0039435495, 0.0040045846, 0.003918118, 0.003975762, 0.003937615, 0.003918118, 0.0039020116, 0.0038786994, 0.003927019, 0.0041321646, 0.0041419137, 0.0040478176, 0.0041388273, 0.0040302277, 0.0041503906], [0.0052083335, 0.0038791234, 0.0039198133, 0.003777398, 0.0038791234, 0.003914727, 0.0038672553, 0.0040334067, 0.003871494, 0.0039015876, 0.0038481818, 0.0038664076, 0.003762563, 0.0037333171, 0.0037367078, 0.0036943224, 0.0037605762, 0.003793776, 0.0037109375], [0.0034722222, 0.0039605033, 0.004163954, 0.0038723415, 0.0039435495, 0.0040062796, 0.0038486058, 0.0039961073, 0.0038799709, 0.0040071276, 0.0039422777, 0.003964742, 0.003990597, 0.004163954, 0.004117754, 0.0041419137, 0.0043013096, 0.0042492747, 0.0037597655], [0.004050926, 0.0034993489, 0.0036756727, 0.0037367078, 0.0039367676, 0.0040011937, 0.004023234, 0.0039689806, 0.003809611, 0.0038795471, 0.0039219325, 0.003804101, 0.003899892, 0.004018148, 0.003965166, 0.0039876304, 0.003996253, 0.0039542317, 0.003930664], [0.0034722222, 0.0039876304, 0.0033908421, 0.004163954, 0.004184299, 0.003757053, 0.0037519666, 0.0038943822, 0.0038570827, 0.0039460924, 0.0038558112, 0.0038074916, 0.0038240221, 0.0037417943, 0.003812154, 0.003748152, 0.0038633347, 0.0039048195, 0.0038818358], [0.0017361111, 0.0038519965, 0.0041503906, 0.0042521157, 0.003774007, 0.0040062796, 0.0040571424, 0.003941854, 0.0038706462, 0.0039829677, 0.004037645, 0.003985087, 0.0039605033, 0.0041393703, 0.0041007996, 0.0041410658, 0.004299283, 0.0042400956, 0.0038330078], [0.0023148148, 0.0035264757, 0.003797743, 0.0040486655, 0.0038248699, 0.0039011638, 0.0038079156, 0.0038570827, 0.0039545693, 0.0039342246, 0.003909217, 0.0037795173, 0.0038765802, 0.003888872, 0.0039248997, 0.0039541456, 0.0037876368, 0.0038760304, 0.003515625], [0.0052083335, 0.0042860243, 0.0042182077, 0.004326714, 0.004055447, 0.003926595, 0.0040317113, 0.003826565, 0.003745185, 0.0037731593, 0.0037540859, 0.0036816066, 0.0036214192, 0.0038223267, 0.003821479, 0.0038231744, 0.004042387, 0.004049897, 0.0036132813], [0.0023148148, 0.004530165, 0.004679362, 0.004313151, 0.0043538413, 0.004341973, 0.0043029785, 0.004536947, 0.004193624, 0.0041503906, 0.0041296217, 0.0041321646, 0.0040096706, 0.0040478176, 0.0040130615, 0.0040774876, 0.003976345, 0.0043046474, 0.0044189454], [0.0023148148, 0.006293403, 0.0054660374, 0.005364312, 0.005001492, 0.0049997964, 0.004906548, 0.0050201416, 0.0047514173, 0.004659441, 0.0046611363, 0.004578484, 0.004579756, 0.004875607, 0.0047916835, 0.0048268638, 0.005153775, 0.0047411323, 0.004711914], [0.004050926, 0.004909939, 0.0047336156, 0.004767524, 0.0044487845, 0.004421658, 0.004284329, 0.004526774, 0.0042529637, 0.0040825736, 0.004041036, 0.0040579904, 0.0039982265, 0.0039422777, 0.0039070975, 0.0038786994, 0.003706336, 0.003782034, 0.0049804687], [0.0028935184, 0.0037163629, 0.004272461, 0.0040690103, 0.003991021, 0.00408766, 0.004055447, 0.003833347, 0.0039172703, 0.003777398, 0.003745185, 0.0038108826, 0.003689236, 0.0038706462, 0.00398212, 0.003936344, 0.004087925, 0.003989041, 0.0040771486], [0.0046296297, 0.0036621094, 0.0038655598, 0.0036349827, 0.003645155, 0.0036417644, 0.0037757026, 0.003660414, 0.0036493938, 0.0035536024, 0.0035103692, 0.0034459431, 0.0035133362, 0.003572252, 0.0035476685, 0.003595564, 0.0036525726, 0.0037794113, 0.0042236326], [0.0028935184, 0.004909939, 0.005086263, 0.0048353407, 0.0045844186, 0.0041469997, 0.0042555067, 0.004062229, 0.0039494834, 0.0038570827, 0.0037875706, 0.003672282, 0.003640069, 0.0041656494, 0.0040334067, 0.004079607, 0.004467249, 0.0038095713, 0.004711914], [0.0046296297, 0.005642361, 0.0056966147, 0.005493164, 0.005662706, 0.0051116943, 0.0048895944, 0.0046776664, 0.0044080946, 0.00440428, 0.004463196, 0.004313575, 0.0042822096, 0.004506853, 0.0046170554, 0.0047158133, 0.0049864054, 0.00452441, 0.004199219], [0.004050926, 0.0063205296, 0.005425347, 0.0050320094, 0.0052524144, 0.0050845677, 0.0049438477, 0.004820082, 0.004536099, 0.0046518114, 0.004562378, 0.004578484, 0.004491594, 0.004930708, 0.00490782, 0.004956987, 0.005413413, 0.004880607, 0.0036621094], [0.0046296297, 0.005018446, 0.0049235024, 0.0047539603, 0.004692925, 0.0047014025, 0.004479302, 0.0045166016, 0.004421658, 0.0042334665, 0.004251268, 0.004228804, 0.004140642, 0.0041478476, 0.0040626526, 0.0041885376, 0.0040961504, 0.004087925, 0.0036132813], [0.0046296297, 0.0042588976, 0.0042860243, 0.0038723415, 0.0040690103, 0.004079183, 0.0040927464, 0.0039961073, 0.0038579304, 0.0038515728, 0.003946516, 0.0038278368, 0.003812154, 0.00398212, 0.003939735, 0.003929562, 0.004036188, 0.004074335, 0.0041503906], [0.004050926, 0.0036621094, 0.0035264757, 0.0037502712, 0.0038384332, 0.003979153, 0.003809611, 0.0037163629, 0.003768921, 0.0037117004, 0.0035684374, 0.0036074321, 0.0035408868, 0.0038439434, 0.003832923, 0.003841824, 0.003930807, 0.0038524866, 0.0040283203], [0.0028935184, 0.0051812064, 0.0051812064, 0.005323622, 0.004760742, 0.004348755, 0.004253811, 0.0042131213, 0.0039452445, 0.0039600795, 0.0038867528, 0.0037684971, 0.003759596, 0.004100376, 0.004099952, 0.004193624, 0.004618764, 0.0039491057, 0.0038330078], [0.0034722222, 0.0051812064, 0.0051405164, 0.004706489, 0.004526774, 0.0046573216, 0.0044301352, 0.004418267, 0.0042614406, 0.004161411, 0.0041563245, 0.0040681628, 0.004090627, 0.0044322545, 0.004336039, 0.0044169957, 0.0046578646, 0.0044441223, 0.0041503906], [0.0052083335, 0.004801432, 0.0050455728, 0.004909939, 0.004679362, 0.0047861733, 0.004638672, 0.00456916, 0.004512363, 0.0044852365, 0.004389021, 0.0043593515, 0.00447464, 0.004776001, 0.004852295, 0.0047823587, 0.005155444, 0.004843354, 0.0043212892], [0.0023148148, 0.004638672, 0.0050455728, 0.0055135093, 0.0050794813, 0.0049947104, 0.0048895944, 0.004655626, 0.0043674046, 0.0043432447, 0.0043228995, 0.00426356, 0.004281362, 0.004447937, 0.0044335257, 0.00448566, 0.0046533346, 0.004378617, 0.004760742], [0.0046296297, 0.0042860243, 0.0052083335, 0.0049302843, 0.0048217773, 0.0045810277, 0.004570855, 0.0043470594, 0.0042232936, 0.004169464, 0.004043579, 0.0040978324, 0.0039867824, 0.004354265, 0.004310184, 0.0043228995, 0.00442636, 0.0040943027, 0.004345703], [0.005787037, 0.004747179, 0.005384657, 0.005445692, 0.0048658582, 0.004626804, 0.0046505397, 0.004316542, 0.003970676, 0.0039253235, 0.003853692, 0.0037723118, 0.0036985609, 0.0041741263, 0.004357232, 0.0042834813, 0.0044584274, 0.003860414, 0.004199219], [0.0046296297, 0.0038519965, 0.0038519965, 0.004211426, 0.0038689508, 0.0042419434, 0.0040418836, 0.0039198133, 0.003929138, 0.0038926867, 0.003826565, 0.0037138197, 0.0037354364, 0.004091051, 0.0040834215, 0.0040690103, 0.004354596, 0.0040415525, 0.0047851563], [0.004050926, 0.0039876304, 0.0040283203, 0.004184299, 0.00442844, 0.004224989, 0.004380968, 0.0041859946, 0.0042970446, 0.004138523, 0.0041181776, 0.004125807, 0.004066891, 0.0041944715, 0.004099528, 0.0041389465, 0.0043919086, 0.004360497, 0.0039550783], [0.0023148148, 0.0055881077, 0.0060356986, 0.005811903, 0.0056118434, 0.0050930446, 0.004928589, 0.004711575, 0.004524231, 0.0045725503, 0.0045433044, 0.0044542947, 0.0044254726, 0.0049510533, 0.004945543, 0.0050315857, 0.0053617954, 0.004762888, 0.003979492], [0.0063657407, 0.0058051217, 0.0054660374, 0.0053575304, 0.0052897134, 0.004740397, 0.004679362, 0.004616631, 0.00455051, 0.0043758815, 0.0043907166, 0.004359775, 0.004275428, 0.0046115452, 0.0045594107, 0.004490746, 0.0049060583, 0.0045095086, 0.0040527345], [0.0063657407, 0.0059136283, 0.0062391493, 0.0057644313, 0.005452474, 0.004986233, 0.004692925, 0.0044233534, 0.0043606227, 0.0042415196, 0.0041741263, 0.004073249, 0.0040249294, 0.004597982, 0.0045750937, 0.0045725503, 0.0047501326, 0.0041409135, 0.0044433596], [0.004050926, 0.0039333766, 0.004014757, 0.004184299, 0.004116482, 0.004279243, 0.004067315, 0.0041334364, 0.0040359497, 0.0039422777, 0.003903283, 0.0038248699, 0.0038231744, 0.0040579904, 0.004027473, 0.0040156045, 0.0040887594, 0.0038981438, 0.0043212892], [0.0023148148, 0.0044759116, 0.004747179, 0.0042385524, 0.0043911403, 0.0041826037, 0.004170736, 0.0040808786, 0.0039452445, 0.00393041, 0.0038295323, 0.0038172405, 0.0037697686, 0.004257202, 0.004184723, 0.0041965907, 0.0044914484, 0.00403893, 0.003930664], [0.0046296297, 0.004964193, 0.0046251086, 0.0045776367, 0.0045132106, 0.0044691297, 0.0042673745, 0.0042419434, 0.0041478476, 0.004194048, 0.004060957, 0.0040579904, 0.0040372214, 0.0042762756, 0.0042593214, 0.004290687, 0.00461936, 0.0043237805, 0.0040283203], [0.008101852, 0.0049913195, 0.005560981, 0.0053575304, 0.004977756, 0.004998101, 0.004725138, 0.004652235, 0.0043563843, 0.0044759116, 0.0043996177, 0.004366981, 0.00440428, 0.0046992833, 0.0046844482, 0.0047213235, 0.0050373077, 0.0047490597, 0.005029297], [0.0046296297, 0.0051812064, 0.0052490234, 0.0053982204, 0.00511339, 0.0050794813, 0.0049591064, 0.004511515, 0.0044877795, 0.0044759116, 0.004497528, 0.0043385825, 0.004283905, 0.0046390956, 0.004688263, 0.0047382778, 0.0050629377, 0.004594326, 0.0036132813], [0.0046296297, 0.0046115452, 0.004272461, 0.0045912, 0.0043877494, 0.0043538413, 0.004389445, 0.004204644, 0.004253811, 0.004231347, 0.0040982566, 0.0040431553, 0.0039719474, 0.0040923227, 0.0041389465, 0.004160987, 0.004143834, 0.004117906, 0.003857422], [0.0028935184, 0.0036621094, 0.004570855, 0.0042521157, 0.0040283203, 0.0042555067, 0.004355537, 0.00412835, 0.0039783055, 0.0040372214, 0.0039986502, 0.0039011638, 0.0038045247, 0.004082998, 0.004057566, 0.0040537515, 0.004195094, 0.004034281, 0.0035888671], [0.0028935184, 0.0044759116, 0.0039876304, 0.0041775173, 0.004384359, 0.0041944715, 0.0042148167, 0.004138523, 0.003994412, 0.003982544, 0.0039020116, 0.0038337708, 0.0038248699, 0.004114363, 0.0041745505, 0.004078759, 0.004335761, 0.004029274, 0.0042236326], [0.0046296297, 0.0045572915, 0.0044623483, 0.0044759116, 0.0044521755, 0.0043640137, 0.0043860544, 0.0041910806, 0.0042292275, 0.0040889317, 0.003967285, 0.004020691, 0.003989326, 0.004302131, 0.004119873, 0.0041770935, 0.004555583, 0.0042336583, 0.0034667968], [0.005787037, 0.0053710938, 0.0056830514, 0.0054796007, 0.0051371255, 0.004837036, 0.0045098197, 0.004470825, 0.0042504203, 0.0044517517, 0.0043233237, 0.0043364633, 0.0042622886, 0.004658169, 0.004529741, 0.004661984, 0.004982829, 0.004627824, 0.0043212892], [0.0017361111, 0.005018446, 0.004760742, 0.0050930446, 0.004574246, 0.0046980116, 0.0045674643, 0.004470825, 0.0043877494, 0.004483117, 0.0043123034, 0.004301707, 0.004278395, 0.004642063, 0.004630195, 0.0046162074, 0.005044222, 0.004726529, 0.0045898436], [0.004050926, 0.004123264, 0.004313151, 0.004204644, 0.004543728, 0.004543728, 0.004377577, 0.0041690404, 0.0042902627, 0.0041241115, 0.004166921, 0.0041092765, 0.0040537515, 0.004167345, 0.004144033, 0.0041469997, 0.004165292, 0.004120469, 0.0041259765], [0.004050926, 0.004340278, 0.004760742, 0.0045572915, 0.004533556, 0.0044335257, 0.0042775474, 0.0042504203, 0.004111396, 0.004090203, 0.004009247, 0.0039486354, 0.003923628, 0.0043415492, 0.0042822096, 0.00427458, 0.004561186, 0.0041310787, 0.004711914], [0.004050926, 0.0047743055, 0.004380968, 0.004543728, 0.0044080946, 0.004421658, 0.00426907, 0.0042351615, 0.0041046143, 0.0040800306, 0.004002465, 0.003902859, 0.0039054023, 0.0043322244, 0.004178789, 0.0042355857, 0.0045456886, 0.004046321, 0.004345703], [0.005787037, 0.0040961374, 0.0042995876, 0.0042453343, 0.004075792, 0.004199558, 0.0042622886, 0.004163954, 0.004079183, 0.004072825, 0.0039889016, 0.0038337708, 0.0038541157, 0.004064772, 0.0040397644, 0.0040868125, 0.004334092, 0.0041400194, 0.004272461], [0.0017361111, 0.0049913195, 0.005493164, 0.005330404, 0.005069309, 0.005021837, 0.004708184, 0.0047268337, 0.0044818455, 0.004497528, 0.004292382, 0.0043110317, 0.0042567784, 0.004550934, 0.00463189, 0.00459671, 0.0048246384, 0.0045987964, 0.004174805], [0.0034722222, 0.0043945312, 0.004869249, 0.00463189, 0.004896376, 0.004725138, 0.004682753, 0.004570855, 0.004333496, 0.004441155, 0.0043665566, 0.0043029785, 0.0043479074, 0.0046306187, 0.004653083, 0.004617903, 0.0049772263, 0.0048221946, 0.0044677733], [0.0034722222, 0.00664605, 0.0062391493, 0.0054592555, 0.005123562, 0.0050116647, 0.004855686, 0.004620022, 0.004418267, 0.004407247, 0.004327562, 0.0041457284, 0.004096985, 0.0044356454, 0.0044377646, 0.0044903224, 0.0045256615, 0.0041499734, 0.004736328], [0.0052083335, 0.0049913195, 0.005059136, 0.004998101, 0.00491333, 0.00460985, 0.00473192, 0.004304674, 0.0042241416, 0.0042415196, 0.004155053, 0.004110972, 0.0039575365, 0.004506005, 0.0045365226, 0.0045229592, 0.0047034025, 0.0041927695, 0.004248047], [0.0028935184, 0.0048285592, 0.005343967, 0.005323622, 0.0048183864, 0.004492866, 0.004514906, 0.004413181, 0.0042122733, 0.0041855704, 0.004061381, 0.004061805, 0.0039172703, 0.004418691, 0.00440979, 0.0044665867, 0.004750967, 0.004122257, 0.0043945312], [0.0017361111, 0.0051812064, 0.004489475, 0.0047268337, 0.0046657985, 0.0044080946, 0.004453871, 0.0042131213, 0.00422838, 0.004216936, 0.0040177237, 0.00405248, 0.003915151, 0.0044021606, 0.004236857, 0.00429323, 0.004651189, 0.0041561127, 0.004199219], [0.0046296297, 0.0057237414, 0.0056830514, 0.0051812064, 0.00519477, 0.0050320094, 0.0046946206, 0.004558987, 0.004362318, 0.0044788783, 0.004289839, 0.004220327, 0.0042326185, 0.0045433044, 0.004597134, 0.0046607126, 0.0049620867, 0.0045748353, 0.004638672], [0.004050926, 0.0050998265, 0.0052083335, 0.0051540798, 0.005211724, 0.0049997964, 0.0048997668, 0.0045725503, 0.004505581, 0.004524231, 0.004483541, 0.0044737924, 0.004413181, 0.004921807, 0.0049090916, 0.0049188402, 0.005353093, 0.0048736334, 0.0040771486], [0.0052083335, 0.0053982204, 0.0052897134, 0.0055881077, 0.0051032174, 0.004996406, 0.0049184165, 0.004708184, 0.0043707956, 0.004430559, 0.0043000113, 0.004324171, 0.004151238, 0.0044542947, 0.004477183, 0.0044318307, 0.0046856403, 0.004335761, 0.0043945312], [0.0028935184, 0.0040418836, 0.004679362, 0.0044962564, 0.004326714, 0.0044267443, 0.004460653, 0.0041792127, 0.0041792127, 0.0041321646, 0.0041571725, 0.004023658, 0.003949059, 0.004243639, 0.004260593, 0.004271189, 0.0042635202, 0.004227221, 0.0042236326], [0.004050926, 0.0042588976, 0.0044487845, 0.004374186, 0.004163954, 0.0042182077, 0.004275852, 0.0041690404, 0.0041300454, 0.003988054, 0.0040219626, 0.0039427015, 0.003943973, 0.0041071572, 0.004162682, 0.0041393703, 0.004199624, 0.00414598, 0.0041015623], [0.0034722222, 0.004692925, 0.0049913195, 0.004964193, 0.004479302, 0.004413181, 0.0044725207, 0.0042131213, 0.004146152, 0.0041864184, 0.0041088527, 0.0040825736, 0.003999498, 0.0043258667, 0.0042915344, 0.0042682225, 0.0046892166, 0.0042178035, 0.004370117], [0.005787037, 0.0056694876, 0.006184896, 0.005486382, 0.0051608616, 0.004816691, 0.0048285592, 0.0045284694, 0.0043724906, 0.004380544, 0.0043097604, 0.004190233, 0.004146152, 0.0044487845, 0.0044208104, 0.004477607, 0.0048389435, 0.0044989586, 0.0044189454], [0.0046296297, 0.0045030382, 0.0051812064, 0.0051540798, 0.0047776964, 0.0048268638, 0.004708184, 0.004614936, 0.004553901, 0.0045085484, 0.0044576856, 0.004379696, 0.004412757, 0.004757775, 0.004760742, 0.004854414, 0.0051492453, 0.004859984, 0.0039550783], [0.0063657407, 0.006184896, 0.0049235024, 0.005384657, 0.0048353407, 0.005133735, 0.0047980417, 0.0046725804, 0.004412333, 0.0044521755, 0.004397074, 0.0042673745, 0.004204644, 0.0047535365, 0.0046123927, 0.0047603184, 0.005007148, 0.0044721365, 0.0038085938], [0.004050926, 0.0040961374, 0.003797743, 0.004401313, 0.004102919, 0.0040808786, 0.0042961966, 0.0042877197, 0.004230923, 0.004178789, 0.0041262307, 0.004016876, 0.0040940177, 0.004139794, 0.004210154, 0.0041474234, 0.004176259, 0.0042636395, 0.0034179688], [0.005787037, 0.0048828125, 0.0047336156, 0.0049031577, 0.004652235, 0.004521688, 0.0045352513, 0.004240248, 0.0041359793, 0.004216512, 0.0041529336, 0.0040813023, 0.0040249294, 0.004342397, 0.004391564, 0.004336887, 0.0045439005, 0.0042114854, 0.004345703], [0.0028935184, 0.0056152344, 0.0056287977, 0.005554199, 0.0049268934, 0.005004883, 0.0048285592, 0.0045386422, 0.004313999, 0.0043258667, 0.004127926, 0.004014757, 0.004067739, 0.004684872, 0.004691654, 0.0046819053, 0.0051379204, 0.0042806864, 0.0043945312], [0.0069444445, 0.005750868, 0.005384657, 0.0048624673, 0.004767524, 0.0050031873, 0.004896376, 0.004620022, 0.004310608, 0.0043953788, 0.004333496, 0.004248301, 0.004145304, 0.004618751, 0.0046539307, 0.0045517813, 0.004996896, 0.004471302, 0.0037353516], [0.0046296297, 0.0048285592, 0.0046115452, 0.0048353407, 0.0045776367, 0.0045403372, 0.0044470895, 0.004479302, 0.00442844, 0.004410214, 0.0043996177, 0.0043212045, 0.004242367, 0.004632314, 0.0046056113, 0.0047208997, 0.004891038, 0.004797697, 0.0043945312], [0.0028935184, 0.00523546, 0.00511339, 0.0050727, 0.005021837, 0.005025228, 0.0048285592, 0.004747179, 0.0043996177, 0.004524231, 0.0044229296, 0.0042703417, 0.004301283, 0.004608578, 0.00465605, 0.0047005545, 0.005010009, 0.004579842, 0.003857422], [0.005787037, 0.00523546, 0.005167643, 0.004896376, 0.004747179, 0.0047200522, 0.0048268638, 0.004416572, 0.004397074, 0.00440428, 0.004316118, 0.0041097007, 0.004120297, 0.004505581, 0.0043936837, 0.0044369167, 0.0046104193, 0.004290104, 0.004199219], [0.0023148148, 0.0052897134, 0.004855686, 0.004909939, 0.0047776964, 0.0046454538, 0.0045166016, 0.004297892, 0.0041088527, 0.004286448, 0.004125807, 0.0040952894, 0.0040334067, 0.0043589273, 0.0043216283, 0.004372067, 0.004618168, 0.0042250156, 0.0045410157], [0.0023148148, 0.0045572915, 0.0046115452, 0.004442003, 0.0043877494, 0.0043538413, 0.0044521755, 0.0041859946, 0.00419871, 0.0042004054, 0.004096561, 0.0041063097, 0.0039401585, 0.004427168, 0.0043343436, 0.004352146, 0.004692793, 0.004270971, 0.0038330078], [0.0017361111, 0.004421658, 0.004652235, 0.004957411, 0.004574246, 0.0045420327, 0.004555596, 0.004470825, 0.0042733084, 0.004306793, 0.004166073, 0.0040927464, 0.004119873, 0.004351722, 0.0043881736, 0.004451328, 0.004714489, 0.0044059157, 0.0045898436], [0.0063657407, 0.0059136283, 0.0051405164, 0.0049235024, 0.0049438477, 0.0047234427, 0.0048302542, 0.0047200522, 0.0044818455, 0.0045344033, 0.0043500266, 0.0042961966, 0.004297892, 0.0046081543, 0.0046602883, 0.0044983756, 0.004853487, 0.00474149, 0.0048828125], [0.005787037, 0.004855686, 0.0045572915, 0.0052625868, 0.0049438477, 0.0048658582, 0.0045776367, 0.004579332, 0.0044750636, 0.004532284, 0.0044377646, 0.004325443, 0.004234314, 0.004636129, 0.0046463013, 0.0046340097, 0.005140424, 0.0046858788, 0.004833984], [0.0017361111, 0.0050727, 0.005533854, 0.0055202907, 0.0050455728, 0.004804823, 0.004804823, 0.004511515, 0.0043792725, 0.0043610465, 0.0041838754, 0.004202101, 0.0041262307, 0.0044898987, 0.004556444, 0.0046221414, 0.0047038794, 0.004268706, 0.004248047], [0.004050926, 0.004123264, 0.0041503906, 0.004326714, 0.0040317113, 0.004357232, 0.0043724906, 0.0042945016, 0.0041181776, 0.0041792127, 0.00412835, 0.0041592917, 0.003981272, 0.0041804845, 0.0042826333, 0.0042936536, 0.004359007, 0.0043125153, 0.0037109375], [0.0023148148, 0.004638672, 0.004326714, 0.0044691297, 0.004377577, 0.004340278, 0.00440979, 0.004284329, 0.0040630763, 0.0041296217, 0.004073673, 0.0040050084, 0.003965166, 0.0043169656, 0.0043322244, 0.00433943, 0.004580617, 0.0042598844, 0.0042236326], [0.0023148148, 0.0044759116, 0.005126953, 0.0046115452, 0.0044691297, 0.0045386422, 0.0046183267, 0.0042961966, 0.004295349, 0.004155053, 0.004138099, 0.004096561, 0.0040694345, 0.0042334665, 0.0042703417, 0.004263984, 0.0045278072, 0.004305005, 0.0041503906], [0.0034722222, 0.0056966147, 0.0059407554, 0.0056559243, 0.005259196, 0.0051320395, 0.0050608316, 0.0047980417, 0.004447937, 0.004620446, 0.0044254726, 0.004379696, 0.004319085, 0.004746331, 0.00475057, 0.004764557, 0.0051363707, 0.004738629, 0.004003906], [0.0046296297, 0.0063205296, 0.005859375, 0.006178114, 0.005238851, 0.0050354004, 0.004986233, 0.0046403673, 0.0045106676, 0.0045229592, 0.004462772, 0.0044309827, 0.0042762756, 0.0049349465, 0.00490782, 0.004934099, 0.005445838, 0.004817724, 0.003857422], [0.0034722222, 0.0049370658, 0.004706489, 0.004536947, 0.004642063, 0.0045352513, 0.0045199925, 0.0043385825, 0.004310608, 0.004258474, 0.0042466056, 0.0041944715, 0.0041542053, 0.0042800903, 0.0043343436, 0.0042355857, 0.00429976, 0.004297316, 0.0044189454], [0.0034722222, 0.0043674046, 0.004380968, 0.0043877494, 0.004333496, 0.0043385825, 0.0044674342, 0.0044403076, 0.004145304, 0.004190233, 0.004238976, 0.004115211, 0.0041410658, 0.0044140285, 0.0043691, 0.0043792725, 0.0045080185, 0.004341483, 0.0041015623], [0.0034722222, 0.0035807292, 0.0038113063, 0.004163954, 0.0041978625, 0.0041097007, 0.0040486655, 0.0041012233, 0.004113091, 0.004016876, 0.004117754, 0.00404697, 0.003953298, 0.0042296518, 0.0041643777, 0.0040974086, 0.004414797, 0.0043453574, 0.0045898436], [0.004050926, 0.004638672, 0.004909939, 0.0047743055, 0.0049913195, 0.004416572, 0.004511515, 0.0045420327, 0.004240248, 0.0043326486, 0.004155477, 0.00411182, 0.004131317, 0.0044585336, 0.0044263205, 0.0043504504, 0.004838705, 0.0043125153, 0.005102539], [0.005787037, 0.005018446, 0.0059950086, 0.005384657, 0.0048251683, 0.0048251683, 0.0046895347, 0.004553901, 0.0043538413, 0.004432678, 0.0043216283, 0.0043356153, 0.004295773, 0.0045132106, 0.004609426, 0.0045339796, 0.004901886, 0.0046650767, 0.003979492], [0.0046296297, 0.005343967, 0.0053982204, 0.0050998265, 0.004848904, 0.0050065783, 0.0047980417, 0.004760742, 0.004576789, 0.004605187, 0.0045038857, 0.0044267443, 0.0043966505, 0.0049552917, 0.004970127, 0.0048946803, 0.0053453445, 0.0048381686, 0.0037353516], [0.0052083335, 0.005533854, 0.005642361, 0.005228678, 0.0050286185, 0.0050269235, 0.004784478, 0.0046861437, 0.0045191445, 0.004419115, 0.004341973, 0.0042856005, 0.004254659, 0.004465739, 0.00448015, 0.0045098197, 0.0047234297, 0.0043600798, 0.0045898436], [0.0034722222, 0.004855686, 0.004652235, 0.0045572915, 0.004543728, 0.004511515, 0.0043216283, 0.0043860544, 0.0043563843, 0.004291958, 0.0041766698, 0.0041156346, 0.0040914747, 0.0044576856, 0.004389445, 0.004327562, 0.004445076, 0.0043709874, 0.00390625], [0.0017361111, 0.0037434895, 0.004638672, 0.0047200522, 0.0043538413, 0.004445394, 0.004477607, 0.004263984, 0.0041419137, 0.004243639, 0.0041677686, 0.00418769, 0.0040872362, 0.004353417, 0.0044373404, 0.004486084, 0.0046218634, 0.0043882728, 0.0041503906], [0.0063657407, 0.004801432, 0.0048828125, 0.004380968, 0.004482693, 0.0045199925, 0.0045895046, 0.00442844, 0.0043284097, 0.004302131, 0.0041516623, 0.0041575963, 0.0040846933, 0.004430559, 0.0044610766, 0.004472097, 0.004792094, 0.0043468475, 0.004248047], [0.0023148148, 0.004231771, 0.004597982, 0.0047539603, 0.0047132703, 0.0045759412, 0.004767524, 0.0044623483, 0.004397074, 0.0043322244, 0.00429323, 0.0042478773, 0.004290687, 0.004486084, 0.0045661926, 0.0045221117, 0.0047769547, 0.004597783, 0.0044921874], [0.0034722222, 0.0042588976, 0.0052083335, 0.005215115, 0.0048760306, 0.0046810573, 0.004769219, 0.004497952, 0.0044462415, 0.0045594107, 0.0045051575, 0.0043877494, 0.004351722, 0.004757775, 0.004796346, 0.0048094857, 0.0051032305, 0.0048880577, 0.004296875], [0.0052083335, 0.005018446, 0.005642361, 0.005174425, 0.005001492, 0.0048624673, 0.0048421226, 0.0045386422, 0.0045106676, 0.00443395, 0.0043758815, 0.004331377, 0.004224989, 0.004603068, 0.0045348275, 0.004565345, 0.0048747063, 0.004481137, 0.004370117], [0.0028935184, 0.0047200522, 0.0049235024, 0.0051540798, 0.0049743652, 0.004755656, 0.004699707, 0.0045233835, 0.0044877795, 0.004346212, 0.004299164, 0.0041368273, 0.0041245352, 0.00443395, 0.0044314065, 0.004441155, 0.0045741796, 0.0043361187, 0.0036376952], [0.005787037, 0.0045572915, 0.0050998265, 0.0047810874, 0.0048387316, 0.0046047634, 0.004470825, 0.004365709, 0.0041588675, 0.0042406716, 0.004200829, 0.0041402183, 0.004018148, 0.0044729444, 0.00442844, 0.0044877795, 0.004704714, 0.0043810606, 0.004272461], [0.005787037, 0.004638672, 0.004964193, 0.0045776367, 0.0044589574, 0.0045250785, 0.004316542, 0.0043996177, 0.0042199027, 0.004216512, 0.0042004054, 0.0041571725, 0.004132589, 0.0044716727, 0.004465739, 0.0044784546, 0.004745126, 0.004391551, 0.0044433596], [0.0028935184, 0.0052897134, 0.0048285592, 0.005364312, 0.004703098, 0.004725138, 0.0045606825, 0.00445048, 0.004555596, 0.0044585336, 0.0043156943, 0.004253811, 0.0041537816, 0.004608578, 0.0045466954, 0.0045780605, 0.004924655, 0.0045693517, 0.004199219], [0.0046296297, 0.0061035156, 0.0061442056, 0.0064629447, 0.0059509277, 0.005210029, 0.0051083034, 0.0049913195, 0.004459805, 0.0047281054, 0.004508972, 0.0044704014, 0.00440979, 0.0049972534, 0.0049786037, 0.005129496, 0.0055150986, 0.004847765, 0.0041259765], [0.0069444445, 0.0051540798, 0.0059814453, 0.0055745444, 0.005499946, 0.0051116943, 0.004928589, 0.0045640734, 0.004532708, 0.004559835, 0.004459381, 0.0043674046, 0.0042126975, 0.004634857, 0.0046217176, 0.0045899283, 0.0050364733, 0.004603505, 0.004248047], [0.0028935184, 0.0047200522, 0.0052490234, 0.005025228, 0.0049336753, 0.0047709146, 0.0047658286, 0.0044589574, 0.0043767295, 0.0044267443, 0.004280514, 0.0042033726, 0.0041817557, 0.0044962564, 0.0044483608, 0.004467858, 0.0047371387, 0.0043867826, 0.0035888671], [0.0046296297, 0.003689236, 0.004231771, 0.0037909613, 0.0040249294, 0.0041775173, 0.00424703, 0.0041215685, 0.004137675, 0.004084269, 0.0040465463, 0.004113091, 0.004038917, 0.0041414895, 0.0042097303, 0.004183451, 0.0043514967, 0.0044050813, 0.00456543], [0.0028935184, 0.0037163629, 0.0045166016, 0.00442844, 0.004326714, 0.004279243, 0.004099528, 0.0044436986, 0.0042504203, 0.0042004054, 0.0041364036, 0.004155477, 0.0040914747, 0.0043737623, 0.004406399, 0.004363166, 0.0046474934, 0.0044261813, 0.00456543], [0.0005787037, 0.0051812064, 0.0050320094, 0.004855686, 0.0047912598, 0.0046437583, 0.004667494, 0.0045250785, 0.0043860544, 0.0043445164, 0.0043987697, 0.004279243, 0.0041542053, 0.004445394, 0.0044784546, 0.004454719, 0.0048857927, 0.0045175552, 0.0041015623], [0.0063657407, 0.005452474, 0.0064697266, 0.00587972, 0.0058831107, 0.0052931043, 0.0050557456, 0.004884508, 0.0046946206, 0.004694197, 0.0044699777, 0.0044962564, 0.0043767295, 0.004921807, 0.0049188402, 0.0049243504, 0.0053595304, 0.004791498, 0.0042236326], [0.0046296297, 0.0055881077, 0.0056559243, 0.005493164, 0.004845513, 0.0049116346, 0.005015055, 0.004526774, 0.004558987, 0.0045047337, 0.0044000414, 0.0044059753, 0.0042945016, 0.004687839, 0.004636129, 0.004817115, 0.005210519, 0.0046945214, 0.004296875], [0.005787037, 0.0047200522, 0.004801432, 0.0045030382, 0.0044148765, 0.0044335257, 0.0045640734, 0.0045352513, 0.0043674046, 0.004231347, 0.004242367, 0.0041724313, 0.0041478476, 0.00431739, 0.0043737623, 0.00433392, 0.0044169426, 0.004360199, 0.00390625], [0.004050926, 0.0047743055, 0.004638672, 0.004977756, 0.0048285592, 0.0045962865, 0.004714966, 0.004465739, 0.0042419434, 0.004442003, 0.0042555067, 0.004272037, 0.0041741263, 0.0046552024, 0.0046170554, 0.004699707, 0.0049242973, 0.004462898, 0.0038085938], [0.0046296297, 0.0044487845, 0.0040418836, 0.004265679, 0.0040486655, 0.0043351916, 0.004403008, 0.0041537816, 0.0042580497, 0.004190657, 0.004152086, 0.0040774876, 0.004148695, 0.004395803, 0.004354689, 0.0043699476, 0.0046257973, 0.0044670105, 0.0048828125], [0.0034722222, 0.004638672, 0.0049913195, 0.0046657985, 0.0045776367, 0.004582723, 0.004740397, 0.0044962564, 0.004307217, 0.0043148464, 0.0042474535, 0.0042334665, 0.0041715833, 0.004539066, 0.0044025844, 0.0044233534, 0.0048485994, 0.00447315, 0.004272461], [0.0052083335, 0.0050455728, 0.004340278, 0.0046047634, 0.004764133, 0.00456916, 0.0046335855, 0.004438612, 0.0044114855, 0.00444497, 0.0044415793, 0.0044335257, 0.004340278, 0.00458569, 0.0046776664, 0.004576365, 0.0049033165, 0.004750788, 0.0044189454], [0.0052083335, 0.004801432, 0.004855686, 0.0048624673, 0.004747179, 0.004558987, 0.004728529, 0.0046488442, 0.0045572915, 0.004428016, 0.004397074, 0.004406823, 0.004291958, 0.0046170554, 0.0046653748, 0.004638672, 0.0051124096, 0.004728377, 0.004174805], [0.0034722222, 0.004801432, 0.004896376, 0.0047946507, 0.005130344, 0.0046115452, 0.0046403673, 0.0045047337, 0.0045394897, 0.0043496024, 0.004324171, 0.004299164, 0.004123264, 0.0043962267, 0.0042487253, 0.0042474535, 0.0044002533, 0.0043628216, 0.003930664], [0.0046296297, 0.0049913195, 0.00527615, 0.004848904, 0.0046488442, 0.0047336156, 0.0046217176, 0.004497952, 0.004307217, 0.0043614707, 0.0043123034, 0.004275004, 0.0041245352, 0.004585266, 0.004530165, 0.004515754, 0.004812479, 0.0044947267, 0.0037109375], [0.0023148148, 0.0048828125, 0.0050455728, 0.0049302843, 0.0047980417, 0.004706489, 0.0045912, 0.0043216283, 0.00422838, 0.0043491786, 0.0042474535, 0.004238976, 0.0041478476, 0.004623413, 0.0045522056, 0.0045721265, 0.0048726797, 0.004483819, 0.004296875], [0.0028935184, 0.004313151, 0.004760742, 0.0048421226, 0.0044759116, 0.0044470895, 0.004438612, 0.0042588976, 0.004365709, 0.004289415, 0.0041864184, 0.004170736, 0.0041181776, 0.004380544, 0.0044140285, 0.0044263205, 0.004700184, 0.0043997765, 0.004296875], [0.0052083335, 0.0043674046, 0.004869249, 0.0044555664, 0.0047098794, 0.0046657985, 0.004635281, 0.0045081247, 0.004530165, 0.004352146, 0.0043076407, 0.0043318006, 0.004286872, 0.004482693, 0.004561954, 0.0044992235, 0.004845619, 0.0047063828, 0.004272461], [0.0063657407, 0.0058051217, 0.005330404, 0.0054050023, 0.0052558053, 0.0049726698, 0.004892985, 0.0046725804, 0.004517449, 0.004611969, 0.0045551723, 0.0045420327, 0.0044356454, 0.0049023097, 0.004915873, 0.0049934387, 0.0054086447, 0.0048416853, 0.0048583983], [0.0063657407, 0.00664605, 0.0061306423, 0.005947537, 0.0058831107, 0.0052778455, 0.0050269235, 0.0048794216, 0.00462087, 0.004580604, 0.0044542947, 0.00443946, 0.004330953, 0.0047340393, 0.0047162375, 0.0047624377, 0.005164027, 0.004425049, 0.003930664], [0.004050926, 0.0051812064, 0.0049913195, 0.0047268337, 0.0049370658, 0.004603068, 0.004696316, 0.004501343, 0.0043224758, 0.004336887, 0.004289415, 0.0041889613, 0.0041355556, 0.004500919, 0.0044415793, 0.004553901, 0.004640579, 0.0044873357, 0.0038330078], [0.004050926, 0.0053710938, 0.0047200522, 0.0049235024, 0.004960802, 0.0047014025, 0.0045606825, 0.0045674643, 0.004319933, 0.004313151, 0.004254235, 0.004210154, 0.004184723, 0.0045280457, 0.0045755175, 0.0046344334, 0.0049254894, 0.004516244, 0.0047851563], [0.0034722222, 0.004014757, 0.0043945312, 0.004380968, 0.0045233835, 0.004401313, 0.004511515, 0.0043758815, 0.004269918, 0.0043356153, 0.004275004, 0.004202101, 0.0041703116, 0.004355537, 0.004380544, 0.0044085183, 0.0047523975, 0.0044307113, 0.004711914], [0.0017361111, 0.0058051217, 0.005601671, 0.005425347, 0.005086263, 0.0047895643, 0.004740397, 0.0046115452, 0.004536947, 0.0044822693, 0.004368252, 0.0044195387, 0.0042495728, 0.0045759412, 0.004536947, 0.004536099, 0.0049322844, 0.0046717525, 0.0045166016], [0.004050926, 0.0051812064, 0.005493164, 0.0050998265, 0.0054151746, 0.0048794216, 0.004881117, 0.004703098, 0.0045988294, 0.0045780605, 0.004541185, 0.0043932595, 0.0044267443, 0.0048781503, 0.0048272875, 0.0049120584, 0.005293727, 0.0048829913, 0.0039550783]]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    train_x, train_y, test_x, test_y = prepare_data()\n",
    "    train_x, test_x = data_preprocessing(train_x, test_x)\n",
    "\n",
    "    # define placeholder x, y_ , keep_prob, learning_rate\n",
    "    x  = tf.placeholder(tf.float32,[None, image_size, image_size, 3])\n",
    "    y_ = tf.placeholder(tf.float32, [None, class_num])\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    learning_rate = tf.placeholder(tf.float32)\n",
    "    train_flag = tf.placeholder(tf.bool)\n",
    "\n",
    "    # build_network\n",
    "    net_weights = {}\n",
    "    \n",
    "    num_weights = 19\n",
    "    K = tf.placeholder(dtype = tf.int32, shape = (num_weights,))\n",
    "    \n",
    "    \n",
    "    net_weights['W_conv1_1'] = {}\n",
    "    net_weights['W_conv1_1']['w'] = tf.get_variable('conv1_1', shape=[3, 3, 3, 64], initializer=tf.contrib.keras.initializers.he_normal())\n",
    "    b_conv1_1 = bias_variable([64])\n",
    "    W_conv1_1_sign = quantization(net_weights['W_conv1_1']['w'],K[0])\n",
    "    output  = tf.nn.relu( batch_norm(conv2d(x,W_conv1_1_sign) + b_conv1_1))\n",
    "    \n",
    "    net_weights['W_conv1_2'] = {}\n",
    "    net_weights['W_conv1_2']['w'] = tf.get_variable('conv1_2', shape=[3, 3, 64, 64], initializer=tf.contrib.keras.initializers.he_normal())\n",
    "    b_conv1_2 = bias_variable([64])\n",
    "    W_conv1_2_sign = quantization(net_weights['W_conv1_2']['w'],K[1])\n",
    "    output  = tf.nn.relu( batch_norm(conv2d(output,W_conv1_2_sign) + b_conv1_2))\n",
    "    output  = max_pool(output, 2, 2, \"pool1\")\n",
    "    \n",
    "    net_weights['W_conv2_1'] = {}\n",
    "    net_weights['W_conv2_1']['w'] = tf.get_variable('conv2_1', shape=[3, 3, 64, 128], initializer=tf.contrib.keras.initializers.he_normal())\n",
    "    b_conv2_1 = bias_variable([128])\n",
    "    W_conv2_1_sign = quantization(net_weights['W_conv2_1']['w'],K[2])\n",
    "    output  = tf.nn.relu( batch_norm(conv2d(output,W_conv2_1_sign) + b_conv2_1))\n",
    "    \n",
    "    net_weights['W_conv2_2'] = {}\n",
    "    net_weights['W_conv2_2']['w'] = tf.get_variable('conv2_2', shape=[3, 3, 128, 128], initializer=tf.contrib.keras.initializers.he_normal())\n",
    "    b_conv2_2 = bias_variable([128])\n",
    "    W_conv2_2_sign = quantization(net_weights['W_conv2_2']['w'],K[3])\n",
    "    output  = tf.nn.relu( batch_norm(conv2d(output,W_conv2_2_sign) + b_conv2_2))\n",
    "    output  = max_pool(output, 2, 2, \"pool2\")\n",
    "    \n",
    "    net_weights['W_conv3_1'] = {}\n",
    "    net_weights['W_conv3_1']['w'] = tf.get_variable('conv3_1', shape=[3, 3, 128, 256], initializer=tf.contrib.keras.initializers.he_normal())\n",
    "    b_conv3_1 = bias_variable([256])\n",
    "    W_conv3_1_sign = quantization(net_weights['W_conv3_1']['w'],K[4])\n",
    "    output  = tf.nn.relu( batch_norm(conv2d(output,W_conv3_1_sign) + b_conv3_1))\n",
    "\n",
    "    net_weights['W_conv3_2'] = {}\n",
    "    net_weights['W_conv3_2']['w'] = tf.get_variable('conv3_2', shape=[3, 3, 256, 256], initializer=tf.contrib.keras.initializers.he_normal())\n",
    "    b_conv3_2 = bias_variable([256])\n",
    "    W_conv3_2_sign = quantization(net_weights['W_conv3_2']['w'],K[5])\n",
    "    output  = tf.nn.relu( batch_norm(conv2d(output,W_conv3_2_sign) + b_conv3_2))\n",
    "\n",
    "    net_weights['W_conv3_3'] = {}\n",
    "    net_weights['W_conv3_3']['w'] = tf.get_variable('conv3_3', shape=[3, 3, 256, 256], initializer=tf.contrib.keras.initializers.he_normal())\n",
    "    b_conv3_3 = bias_variable([256])\n",
    "    W_conv3_3_sign = quantization(net_weights['W_conv3_3']['w'],K[6])\n",
    "    output  = tf.nn.relu( batch_norm(conv2d(output,W_conv3_3_sign) + b_conv3_3))\n",
    "\n",
    "    net_weights['W_conv3_4'] = {}\n",
    "    net_weights['W_conv3_4']['w'] = tf.get_variable('conv3_4', shape=[3, 3, 256, 256], initializer=tf.contrib.keras.initializers.he_normal())\n",
    "    b_conv3_4 = bias_variable([256])\n",
    "    W_conv3_4_sign = quantization(net_weights['W_conv3_4']['w'],K[7])\n",
    "    output  = tf.nn.relu( batch_norm(conv2d(output,W_conv3_4_sign) + b_conv3_4))\n",
    "    output  = max_pool(output, 2, 2, \"pool3\")\n",
    "\n",
    "    net_weights['W_conv4_1'] = {}\n",
    "    net_weights['W_conv4_1']['w'] = tf.get_variable('conv4_1', shape=[3, 3, 256, 512], initializer=tf.contrib.keras.initializers.he_normal())\n",
    "    b_conv4_1 = bias_variable([512])\n",
    "    W_conv4_1_sign = quantization(net_weights['W_conv4_1']['w'],K[8])\n",
    "    output  = tf.nn.relu( batch_norm(conv2d(output,W_conv4_1_sign) + b_conv4_1))\n",
    "\n",
    "    net_weights['W_conv4_2'] = {}\n",
    "    net_weights['W_conv4_2']['w'] = tf.get_variable('conv4_2', shape=[3, 3, 512, 512], initializer=tf.contrib.keras.initializers.he_normal())\n",
    "    b_conv4_2 = bias_variable([512])\n",
    "    W_conv4_2_sign = quantization(net_weights['W_conv4_2']['w'],K[9])\n",
    "    output  = tf.nn.relu( batch_norm(conv2d(output,W_conv4_2_sign) + b_conv4_2))\n",
    "    \n",
    "    net_weights['W_conv4_3'] = {}\n",
    "    net_weights['W_conv4_3']['w'] = tf.get_variable('conv4_3', shape=[3, 3, 512, 512], initializer=tf.contrib.keras.initializers.he_normal())\n",
    "    b_conv4_3 = bias_variable([512])\n",
    "    W_conv4_3_sign = quantization(net_weights['W_conv4_3']['w'],K[10])\n",
    "    output  = tf.nn.relu( batch_norm(conv2d(output,W_conv4_3_sign) + b_conv4_3))\n",
    "    \n",
    "    net_weights['W_conv4_4'] = {}\n",
    "    net_weights['W_conv4_4']['w'] = tf.get_variable('conv4_4', shape=[3, 3, 512, 512], initializer=tf.contrib.keras.initializers.he_normal())\n",
    "    b_conv4_4 = bias_variable([512])\n",
    "    W_conv4_4_sign = quantization(net_weights['W_conv4_4']['w'],K[11])\n",
    "    output  = tf.nn.relu( batch_norm(conv2d(output,W_conv4_4_sign)) + b_conv4_4)\n",
    "    output  = max_pool(output, 2, 2)\n",
    "    \n",
    "    net_weights['W_conv5_1'] = {}\n",
    "    net_weights['W_conv5_1']['w'] = tf.get_variable('conv5_1', shape=[3, 3, 512, 512], initializer=tf.contrib.keras.initializers.he_normal())\n",
    "    b_conv5_1 = bias_variable([512])\n",
    "    W_conv5_1_sign = quantization(net_weights['W_conv5_1']['w'],K[12])\n",
    "    output  = tf.nn.relu( batch_norm(conv2d(output,W_conv5_1_sign) + b_conv5_1))\n",
    "    \n",
    "    net_weights['W_conv5_2'] = {}\n",
    "    net_weights['W_conv5_2']['w']= tf.get_variable('conv5_2', shape=[3, 3, 512, 512], initializer=tf.contrib.keras.initializers.he_normal())\n",
    "    b_conv5_2 = bias_variable([512])\n",
    "    W_conv5_2_sign = quantization(net_weights['W_conv5_2']['w'],K[13])\n",
    "    output  = tf.nn.relu( batch_norm(conv2d(output,W_conv5_2_sign) + b_conv5_2))\n",
    "    \n",
    "    net_weights['W_conv5_3'] = {}\n",
    "    net_weights['W_conv5_3']['w'] = tf.get_variable('conv5_3', shape=[3, 3, 512, 512], initializer=tf.contrib.keras.initializers.he_normal())\n",
    "    b_conv5_3 = bias_variable([512])\n",
    "    W_conv5_3_sign = quantization(net_weights['W_conv5_3']['w'],K[14])\n",
    "    output  = tf.nn.relu( batch_norm(conv2d(output,W_conv5_3_sign) + b_conv5_3))\n",
    "    \n",
    "    net_weights['W_conv5_4'] = {}\n",
    "    net_weights['W_conv5_4']['w'] = tf.get_variable('conv5_4', shape=[3, 3, 512, 512], initializer=tf.contrib.keras.initializers.he_normal())\n",
    "    b_conv5_4 = bias_variable([512])\n",
    "    W_conv5_4_sign = quantization(net_weights['W_conv5_4']['w'],K[15])\n",
    "    output  = tf.nn.relu( batch_norm(conv2d(output,W_conv5_4_sign) + b_conv5_4))\n",
    "\n",
    "    # output = tf.contrib.layers.flatten(output)\n",
    "    output = tf.reshape(output,[-1,2*2*512])\n",
    "    \n",
    "    net_weights['W_fc1'] = {}\n",
    "    net_weights['W_fc1']['w'] = tf.get_variable('fc1', shape=[2048,4096], initializer=tf.contrib.keras.initializers.he_normal())\n",
    "    b_fc1 = bias_variable([4096])\n",
    "    W_fc1_sign = quantization(net_weights['W_fc1']['w'],K[16])\n",
    "    output = tf.nn.relu( batch_norm(tf.matmul(output,W_fc1_sign) + b_fc1) )\n",
    "    output  = tf.nn.dropout(output,keep_prob)\n",
    "    \n",
    "    net_weights['W_fc2'] = {}\n",
    "    net_weights['W_fc2']['w'] = tf.get_variable('fc7', shape=[4096,4096], initializer=tf.contrib.keras.initializers.he_normal())\n",
    "    b_fc2 = bias_variable([4096])\n",
    "    W_fc2_sign = quantization(net_weights['W_fc2']['w'],K[17])\n",
    "    output = tf.nn.relu( batch_norm(tf.matmul(output,W_fc2_sign) + b_fc2) )\n",
    "    output  = tf.nn.dropout(output,keep_prob)\n",
    "\n",
    "    net_weights['W_fc3'] = {}\n",
    "    net_weights['W_fc3']['w'] = tf.get_variable('fc3', shape=[4096,10], initializer=tf.contrib.keras.initializers.he_normal())\n",
    "    b_fc3 = bias_variable([10])\n",
    "    W_fc3_sign = quantization(net_weights['W_fc3']['w'],K[18])\n",
    "    output = tf.nn.relu( batch_norm(tf.matmul(output,W_fc3_sign) + b_fc3) )\n",
    "    # output  = tf.reshape(output,[-1,10])\n",
    "\n",
    "    reg_loss = 0\n",
    "    weights = []\n",
    "    for key in net_weights.keys():\n",
    "        weights.append(net_weights[key]['w'])\n",
    "#     weights = [W_conv1_1,W_conv1_2,W_conv2_1,W_conv2_2,W_conv3_1,W_conv3_2,W_conv3_3,W_conv3_4,W_conv4_1,W_conv4_1,W_conv4_2,W_conv4_3,W_conv4_4,W_conv5_1,W_conv5_2,W_conv5_3,W_conv5_4,W_fc1,W_fc2,W_fc3]\n",
    "    # for weight in weights:\n",
    "    #     reg_loss+= tf.reduce_sum(tf.abs(tf.add(tf.ones_like(weight),tf.negative(tf.square(weight)))))\n",
    "    reg_loss = reg_loss_fn(net_weights['W_conv1_1']['w'], K[0]) \\\n",
    "            +  reg_loss_fn(net_weights['W_conv1_2']['w'], K[1]) \\\n",
    "            +  reg_loss_fn(net_weights['W_conv2_1']['w'], K[2]) \\\n",
    "            +  reg_loss_fn(net_weights['W_conv2_2']['w'], K[3]) \\\n",
    "            +  reg_loss_fn(net_weights['W_conv3_1']['w'], K[4]) \\\n",
    "            +  reg_loss_fn(net_weights['W_conv3_2']['w'], K[5]) \\\n",
    "            +  reg_loss_fn(net_weights['W_conv3_3']['w'], K[6]) \\\n",
    "            +  reg_loss_fn(net_weights['W_conv3_4']['w'], K[7]) \\\n",
    "            +  reg_loss_fn(net_weights['W_conv4_1']['w'], K[8]) \\\n",
    "            +  reg_loss_fn(net_weights['W_conv4_2']['w'], K[9]) \\\n",
    "            +  reg_loss_fn(net_weights['W_conv4_3']['w'], K[10]) \\\n",
    "            +  reg_loss_fn(net_weights['W_conv4_4']['w'], K[11]) \\\n",
    "            +  reg_loss_fn(net_weights['W_conv5_1']['w'], K[12]) \\\n",
    "            +  reg_loss_fn(net_weights['W_conv5_2']['w'], K[13]) \\\n",
    "            +  reg_loss_fn(net_weights['W_conv5_3']['w'], K[14]) \\\n",
    "            +  reg_loss_fn(net_weights['W_conv5_4']['w'], K[15]) \\\n",
    "            +  reg_loss_fn(net_weights['W_fc1']['w'],K[16]) \\\n",
    "            +  reg_loss_fn(net_weights['W_fc2']['w'],K[17]) \\\n",
    "            +  reg_loss_fn(net_weights['W_fc3']['w'],K[18]) \\\n",
    "\n",
    "\n",
    "\n",
    "    # loss function: cross_entropy\n",
    "    # train_step: training operation\n",
    "    cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=output))\n",
    "    l2 = tf.add_n([tf.nn.l2_loss(var) for var in tf.trainable_variables()])\n",
    "    lbda = tf.placeholder(tf.float32)\n",
    "    train_step = tf.train.MomentumOptimizer(learning_rate, momentum_rate,use_nesterov=True).minimize(cross_entropy + lbda*reg_loss)\n",
    "    \n",
    "    correct_prediction = tf.equal(tf.argmax(output,1), tf.argmax(y_,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
    "\n",
    "    #Masks\n",
    "    epsilon = 0.00001\n",
    "    mask = [mask_fn(weights[i],K[i],epsilon) for i in range(len(weights))]\n",
    "    \n",
    "    # epsilon = 0.00001\n",
    "    # for i in weights:\n",
    "    #     mask.append(tf.reduce_mean(tf.abs(tf.where(tf.less(tf.abs(tf.subtract(tf.abs(i),tf.ones_like(i))),epsilon),tf.sign(i), tf.zeros_like(i)))))\n",
    "\n",
    "\n",
    "    #Binarization \n",
    "    binary = []\n",
    "    for i in weights:\n",
    "        binary.append(tf.assign(i, tf.sign(i)))\n",
    "\n",
    "\n",
    "    list_bin = []\n",
    "\n",
    "\n",
    "    # initial an saver to save model\n",
    "    saver = tf.train.Saver()\n",
    "    config=tf.ConfigProto(allow_soft_placement=True, log_device_placement=True)\n",
    "    config.gpu_options.allow_growth = True\n",
    "    with tf.Session(config = config) as sess:\n",
    "        \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        summary_writer = tf.summary.FileWriter(log_save_path,sess.graph)\n",
    "\n",
    "        # epoch = 164 \n",
    "        # make sure [bath_size * iteration = data_set_number]\n",
    "        k_list = []\n",
    "        for i in range(num_weights):\n",
    "            k_list.append(1)\n",
    "        for ep in range(1,total_epoch+1):\n",
    "            lr = learning_rate_schedule(ep%50)\n",
    "            pre_index = 0\n",
    "            train_acc = 0.0\n",
    "            train_loss = 0.0\n",
    "            start_time = time.time()\n",
    "            print(\"\\nepoch %d/%d:\" %(ep,total_epoch))\n",
    "            masks = None\n",
    "            for it in range(1,iterations+1):\n",
    "                batch_x = train_x[pre_index:pre_index+batch_size]\n",
    "                batch_y = train_y[pre_index:pre_index+batch_size]\n",
    "\n",
    "                batch_x = data_augmentation(batch_x)\n",
    "\n",
    "                _, batch_loss = sess.run([train_step, cross_entropy],feed_dict={x:batch_x, y_:batch_y, keep_prob: dropout_rate, learning_rate: lr, train_flag: True, lbda:0.01, K:k_list})\n",
    "                batch_acc = accuracy.eval(feed_dict={x:batch_x, y_:batch_y, keep_prob: 1.0, train_flag: True, K:k_list})\n",
    "\n",
    "                train_loss += batch_loss\n",
    "                train_acc  += batch_acc\n",
    "                pre_index  += batch_size\n",
    "\n",
    "                if it == iterations:\n",
    "                    train_loss /= iterations\n",
    "                    train_acc /= iterations\n",
    "\n",
    "                    loss_, acc_  = sess.run([cross_entropy,accuracy],feed_dict={x:batch_x, y_:batch_y, keep_prob: 1.0, train_flag: True, lbda:0.01, K:k_list})\n",
    "                    train_summary = tf.Summary(value=[tf.Summary.Value(tag=\"train_loss\", simple_value=train_loss), \n",
    "                                          tf.Summary.Value(tag=\"train_accuracy\", simple_value=train_acc)])\n",
    "                    val_acc, val_loss, test_summary = run_testing(sess,ep,k_list)\n",
    "\n",
    "                    summary_writer.add_summary(train_summary, ep)\n",
    "                    summary_writer.add_summary(test_summary, ep)\n",
    "                    summary_writer.flush()\n",
    "\n",
    "                    print(\"iteration: %d/%d, cost_time: %ds, train_loss: %.4f, train_acc: %.4f, test_loss: %.4f, test_acc: %.4f\" %(it, iterations, int(time.time()-start_time), train_loss, train_acc, val_loss, val_acc))\n",
    "                    list_bin.append(sess.run(mask, feed_dict = {K:k_list}))\n",
    "                    masks = sess.run(mask, feed_dict = {K:k_list})\n",
    "#                     print (sess.run(weights))\n",
    "                else:\n",
    "                    print(\"iteration: %d/%d, train_loss: %.4f, train_acc: %.4f\" %(it, iterations, train_loss / it, train_acc / it) , end='\\r')\n",
    "            if(ep%50 == 0):\n",
    "                print(masks)\n",
    "                k_list = update_k(masks,k_list)\n",
    "            print (k_list)\n",
    "        save_path = saver.save(sess, model_save_path)\n",
    "        print(\"Model saved in file: %s\" % save_path)\n",
    "        print (list_bin)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
